{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGK0Gnbasmln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac13b058-93a4-4457-efa3-059611993b10"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\t#re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\t#line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\t#line = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\t#line = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\t#line = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\t#line = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = '/content/ara.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-arabic.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-arabic.pkl\n",
            "[Hi] => [مرحبًا]\n",
            "[Run] => [اركض]\n",
            "[Help] => [النجدة]\n",
            "[Jump] => [اقفز]\n",
            "[Stop] => [قف]\n",
            "[Go on] => [داوم]\n",
            "[Go on] => [استمر]\n",
            "[Hello] => [مرحباً]\n",
            "[Hurry] => [تعجّل]\n",
            "[Hurry] => [استعجل]\n",
            "[I see] => [انا اري]\n",
            "[I won] => [أنا فُزت]\n",
            "[Relax] => [استرح]\n",
            "[Smile] => [ابتسم]\n",
            "[Cheers] => [في صحتك]\n",
            "[Got it] => [هل فهمت؟]\n",
            "[He ran] => [ركض]\n",
            "[I know] => [أعرف]\n",
            "[I know] => [أعلم ذلك]\n",
            "[I know] => [أنا أعلم]\n",
            "[Im 19] => [أنا في 19]\n",
            "[Im OK] => [أنا بخير]\n",
            "[Listen] => [استمع]\n",
            "[No way] => [غير معقول]\n",
            "[Really] => [حقاً؟]\n",
            "[Thanks] => [شكرا]\n",
            "[Why me] => [لماذا أنا؟]\n",
            "[Awesome] => [رائع]\n",
            "[Be cool] => [خذ راحتك]\n",
            "[Call me] => [هاتفني]\n",
            "[Call me] => [اتصل بي]\n",
            "[Come in] => [تفضل بالدخول]\n",
            "[Come in] => [تعال إلى الداخل]\n",
            "[Come on] => [بالله عليك]\n",
            "[Come on] => [هيا]\n",
            "[Come on] => [هيّا]\n",
            "[Get out] => [اخرج من هنا]\n",
            "[Get out] => [أُخرج]\n",
            "[Get out] => [اخرج]\n",
            "[Go away] => [اتركني و شأني]\n",
            "[Go away] => [اذهب بعيداً]\n",
            "[Go away] => [ارحل]\n",
            "[Goodbye] => [مع السلامة]\n",
            "[He came] => [لقد أتى]\n",
            "[He runs] => [هو يجري]\n",
            "[Help me] => [ساعدني]\n",
            "[Help me] => [النجدة ساعدني]\n",
            "[Im sad] => [أنا حزين]\n",
            "[Me too] => [أنا أيضاً]\n",
            "[Shut up] => [اخرس]\n",
            "[Shut up] => [اصمت]\n",
            "[Shut up] => [اسكت]\n",
            "[Shut up] => [أغلق فمك]\n",
            "[Stop it] => [أوقفه]\n",
            "[Take it] => [خذه]\n",
            "[Tell me] => [أخبرني]\n",
            "[Tom won] => [توم فاز]\n",
            "[Tom won] => [لقد ربح توم]\n",
            "[Wake up] => [استيقظ]\n",
            "[Welcome] => [أهلاً و سهلاً]\n",
            "[Welcome] => [مرحباً بك]\n",
            "[Welcome] => [اهلا وسهلا]\n",
            "[Welcome] => [مرحبا]\n",
            "[Who won] => [من فاز؟]\n",
            "[Who won] => [من الذي ربح؟]\n",
            "[Why not] => [لم لا؟]\n",
            "[Why not] => [لما لا؟]\n",
            "[Have fun] => [استمتع بوقتك]\n",
            "[Hurry up] => [أسرع]\n",
            "[I forgot] => [لقد نسيت]\n",
            "[I got it] => [فهمتُهُ]\n",
            "[I got it] => [فهمتُها]\n",
            "[I got it] => [فَهمتُ ذلك]\n",
            "[I use it] => [أستخدمه]\n",
            "[Ill pay] => [سأدفع أنا]\n",
            "[Im busy] => [أنا مشغول]\n",
            "[Im busy] => [إنني مشغول]\n",
            "[Im cold] => [أشعر بالبرد]\n",
            "[Im free] => [أنا حُرّ]\n",
            "[Im here] => [أنا هنا]\n",
            "[Im home] => [لقد عدت إلى البيت]\n",
            "[Im poor] => [أنا فقير]\n",
            "[Im rich] => [أنا ثري]\n",
            "[It hurts] => [هذا مؤلم]\n",
            "[Its hot] => [الجو حار]\n",
            "[Its new] => [إنه جديد]\n",
            "[Lets go] => [هيا بنا]\n",
            "[Lets go] => [هيا لنذهب]\n",
            "[Lets go] => [لنذهب]\n",
            "[Lets go] => [هيا بنا]\n",
            "[Lets go] => [هيا بنا نذهب]\n",
            "[Look out] => [اِنتبه]\n",
            "[Look out] => [إحذر]\n",
            "[Look out] => [انتبه]\n",
            "[Speak up] => [تكلم]\n",
            "[Stand up] => [قف]\n",
            "[Terrific] => [رائع]\n",
            "[Terrific] => [ممتاز]\n",
            "[Tom died] => [توم مات]\n",
            "[Tom died] => [توفي توم]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kexNyVjmtE26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2c07e14-43eb-43e1-c158-73825fb4da07"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-arabic.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 10000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:9000], dataset[1000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-arabic-both.pkl')\n",
        "save_clean_data(train, 'english-arabic-train.pkl')\n",
        "save_clean_data(test, 'english-arabic-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-arabic-both.pkl\n",
            "Saved: english-arabic-train.pkl\n",
            "Saved: english-arabic-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wP_oNkhItOfq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "877e128a-967d-4cd4-ec04-5e035a833170"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-arabic-both.pkl')\n",
        "train = load_clean_sentences('english-arabic-train.pkl')\n",
        "test = load_clean_sentences('english-arabic-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Arabic Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Arabic Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 3523\n",
            "English Max Length: 10\n",
            "Arabic Vocabulary Size: 10274\n",
            "Arabic Max Length: 14\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 14, 256)           2630144   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 10, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 10, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 10, 3523)          905411    \n",
            "=================================================================\n",
            "Total params: 4,586,179\n",
            "Trainable params: 4,586,179\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "141/141 - 61s - loss: 4.1921 - val_loss: 3.3947\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.39465, saving model to model.h5\n",
            "Epoch 2/100\n",
            "141/141 - 54s - loss: 3.2887 - val_loss: 3.1690\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.39465 to 3.16903, saving model to model.h5\n",
            "Epoch 3/100\n",
            "141/141 - 53s - loss: 3.1584 - val_loss: 3.0800\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.16903 to 3.07998, saving model to model.h5\n",
            "Epoch 4/100\n",
            "141/141 - 54s - loss: 3.0769 - val_loss: 3.0229\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.07998 to 3.02293, saving model to model.h5\n",
            "Epoch 5/100\n",
            "141/141 - 54s - loss: 3.0102 - val_loss: 2.9667\n",
            "\n",
            "Epoch 00005: val_loss improved from 3.02293 to 2.96668, saving model to model.h5\n",
            "Epoch 6/100\n",
            "141/141 - 55s - loss: 2.9628 - val_loss: 2.9294\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.96668 to 2.92939, saving model to model.h5\n",
            "Epoch 7/100\n",
            "141/141 - 55s - loss: 2.9262 - val_loss: 2.9017\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.92939 to 2.90168, saving model to model.h5\n",
            "Epoch 8/100\n",
            "141/141 - 54s - loss: 2.8930 - val_loss: 2.8731\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.90168 to 2.87311, saving model to model.h5\n",
            "Epoch 9/100\n",
            "141/141 - 54s - loss: 2.8574 - val_loss: 2.8325\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.87311 to 2.83248, saving model to model.h5\n",
            "Epoch 10/100\n",
            "141/141 - 55s - loss: 2.7966 - val_loss: 2.7630\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.83248 to 2.76303, saving model to model.h5\n",
            "Epoch 11/100\n",
            "141/141 - 65s - loss: 2.7235 - val_loss: 2.6911\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.76303 to 2.69111, saving model to model.h5\n",
            "Epoch 12/100\n",
            "141/141 - 54s - loss: 2.6561 - val_loss: 2.6322\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.69111 to 2.63222, saving model to model.h5\n",
            "Epoch 13/100\n",
            "141/141 - 54s - loss: 2.5917 - val_loss: 2.5702\n",
            "\n",
            "Epoch 00013: val_loss improved from 2.63222 to 2.57024, saving model to model.h5\n",
            "Epoch 14/100\n",
            "141/141 - 54s - loss: 2.5202 - val_loss: 2.5064\n",
            "\n",
            "Epoch 00014: val_loss improved from 2.57024 to 2.50643, saving model to model.h5\n",
            "Epoch 15/100\n",
            "141/141 - 55s - loss: 2.4550 - val_loss: 2.4464\n",
            "\n",
            "Epoch 00015: val_loss improved from 2.50643 to 2.44635, saving model to model.h5\n",
            "Epoch 16/100\n",
            "141/141 - 54s - loss: 2.3846 - val_loss: 2.3868\n",
            "\n",
            "Epoch 00016: val_loss improved from 2.44635 to 2.38676, saving model to model.h5\n",
            "Epoch 17/100\n",
            "141/141 - 54s - loss: 2.3120 - val_loss: 2.3098\n",
            "\n",
            "Epoch 00017: val_loss improved from 2.38676 to 2.30985, saving model to model.h5\n",
            "Epoch 18/100\n",
            "141/141 - 55s - loss: 2.2408 - val_loss: 2.2473\n",
            "\n",
            "Epoch 00018: val_loss improved from 2.30985 to 2.24728, saving model to model.h5\n",
            "Epoch 19/100\n",
            "141/141 - 55s - loss: 2.1628 - val_loss: 2.1724\n",
            "\n",
            "Epoch 00019: val_loss improved from 2.24728 to 2.17241, saving model to model.h5\n",
            "Epoch 20/100\n",
            "141/141 - 55s - loss: 2.0782 - val_loss: 2.0940\n",
            "\n",
            "Epoch 00020: val_loss improved from 2.17241 to 2.09404, saving model to model.h5\n",
            "Epoch 21/100\n",
            "141/141 - 55s - loss: 1.9956 - val_loss: 2.0145\n",
            "\n",
            "Epoch 00021: val_loss improved from 2.09404 to 2.01449, saving model to model.h5\n",
            "Epoch 22/100\n",
            "141/141 - 56s - loss: 1.9190 - val_loss: 1.9445\n",
            "\n",
            "Epoch 00022: val_loss improved from 2.01449 to 1.94453, saving model to model.h5\n",
            "Epoch 23/100\n",
            "141/141 - 59s - loss: 1.8365 - val_loss: 1.8796\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.94453 to 1.87956, saving model to model.h5\n",
            "Epoch 24/100\n",
            "141/141 - 55s - loss: 1.7603 - val_loss: 1.8041\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.87956 to 1.80412, saving model to model.h5\n",
            "Epoch 25/100\n",
            "141/141 - 54s - loss: 1.6774 - val_loss: 1.7283\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.80412 to 1.72832, saving model to model.h5\n",
            "Epoch 26/100\n",
            "141/141 - 55s - loss: 1.5981 - val_loss: 1.6606\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.72832 to 1.66057, saving model to model.h5\n",
            "Epoch 27/100\n",
            "141/141 - 55s - loss: 1.5219 - val_loss: 1.6003\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.66057 to 1.60035, saving model to model.h5\n",
            "Epoch 28/100\n",
            "141/141 - 55s - loss: 1.4519 - val_loss: 1.5441\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.60035 to 1.54412, saving model to model.h5\n",
            "Epoch 29/100\n",
            "141/141 - 54s - loss: 1.3831 - val_loss: 1.4799\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.54412 to 1.47987, saving model to model.h5\n",
            "Epoch 30/100\n",
            "141/141 - 53s - loss: 1.3176 - val_loss: 1.4165\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.47987 to 1.41654, saving model to model.h5\n",
            "Epoch 31/100\n",
            "141/141 - 54s - loss: 1.2484 - val_loss: 1.3602\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.41654 to 1.36024, saving model to model.h5\n",
            "Epoch 32/100\n",
            "141/141 - 54s - loss: 1.1868 - val_loss: 1.3238\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.36024 to 1.32384, saving model to model.h5\n",
            "Epoch 33/100\n",
            "141/141 - 54s - loss: 1.1280 - val_loss: 1.2637\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.32384 to 1.26370, saving model to model.h5\n",
            "Epoch 34/100\n",
            "141/141 - 53s - loss: 1.0682 - val_loss: 1.2151\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.26370 to 1.21508, saving model to model.h5\n",
            "Epoch 35/100\n",
            "141/141 - 54s - loss: 1.0131 - val_loss: 1.1611\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.21508 to 1.16115, saving model to model.h5\n",
            "Epoch 36/100\n",
            "141/141 - 54s - loss: 0.9545 - val_loss: 1.1199\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.16115 to 1.11986, saving model to model.h5\n",
            "Epoch 37/100\n",
            "141/141 - 54s - loss: 0.9054 - val_loss: 1.0790\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.11986 to 1.07903, saving model to model.h5\n",
            "Epoch 38/100\n",
            "141/141 - 54s - loss: 0.8565 - val_loss: 1.0447\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.07903 to 1.04467, saving model to model.h5\n",
            "Epoch 39/100\n",
            "141/141 - 55s - loss: 0.8101 - val_loss: 0.9990\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.04467 to 0.99901, saving model to model.h5\n",
            "Epoch 40/100\n",
            "141/141 - 54s - loss: 0.7626 - val_loss: 0.9654\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.99901 to 0.96537, saving model to model.h5\n",
            "Epoch 41/100\n",
            "141/141 - 56s - loss: 0.7198 - val_loss: 0.9302\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.96537 to 0.93024, saving model to model.h5\n",
            "Epoch 42/100\n",
            "141/141 - 55s - loss: 0.6814 - val_loss: 0.9066\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.93024 to 0.90659, saving model to model.h5\n",
            "Epoch 43/100\n",
            "141/141 - 54s - loss: 0.6440 - val_loss: 0.8695\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.90659 to 0.86947, saving model to model.h5\n",
            "Epoch 44/100\n",
            "141/141 - 55s - loss: 0.6030 - val_loss: 0.8408\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.86947 to 0.84083, saving model to model.h5\n",
            "Epoch 45/100\n",
            "141/141 - 55s - loss: 0.5658 - val_loss: 0.8083\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.84083 to 0.80834, saving model to model.h5\n",
            "Epoch 46/100\n",
            "141/141 - 55s - loss: 0.5339 - val_loss: 0.7843\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.80834 to 0.78429, saving model to model.h5\n",
            "Epoch 47/100\n",
            "141/141 - 54s - loss: 0.5055 - val_loss: 0.7682\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.78429 to 0.76823, saving model to model.h5\n",
            "Epoch 48/100\n",
            "141/141 - 55s - loss: 0.4785 - val_loss: 0.7439\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.76823 to 0.74386, saving model to model.h5\n",
            "Epoch 49/100\n",
            "141/141 - 55s - loss: 0.4494 - val_loss: 0.7201\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.74386 to 0.72013, saving model to model.h5\n",
            "Epoch 50/100\n",
            "141/141 - 55s - loss: 0.4219 - val_loss: 0.6993\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.72013 to 0.69933, saving model to model.h5\n",
            "Epoch 51/100\n",
            "141/141 - 56s - loss: 0.3959 - val_loss: 0.6800\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.69933 to 0.68002, saving model to model.h5\n",
            "Epoch 52/100\n",
            "141/141 - 55s - loss: 0.3703 - val_loss: 0.6593\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.68002 to 0.65926, saving model to model.h5\n",
            "Epoch 53/100\n",
            "141/141 - 55s - loss: 0.3459 - val_loss: 0.6424\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.65926 to 0.64237, saving model to model.h5\n",
            "Epoch 54/100\n",
            "141/141 - 56s - loss: 0.3276 - val_loss: 0.6345\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.64237 to 0.63450, saving model to model.h5\n",
            "Epoch 55/100\n",
            "141/141 - 56s - loss: 0.3088 - val_loss: 0.6172\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.63450 to 0.61723, saving model to model.h5\n",
            "Epoch 56/100\n",
            "141/141 - 55s - loss: 0.2889 - val_loss: 0.6051\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.61723 to 0.60511, saving model to model.h5\n",
            "Epoch 57/100\n",
            "141/141 - 56s - loss: 0.2708 - val_loss: 0.5954\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.60511 to 0.59538, saving model to model.h5\n",
            "Epoch 58/100\n",
            "141/141 - 56s - loss: 0.2543 - val_loss: 0.5772\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.59538 to 0.57723, saving model to model.h5\n",
            "Epoch 59/100\n",
            "141/141 - 55s - loss: 0.2376 - val_loss: 0.5751\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.57723 to 0.57508, saving model to model.h5\n",
            "Epoch 60/100\n",
            "141/141 - 54s - loss: 0.2285 - val_loss: 0.5684\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.57508 to 0.56843, saving model to model.h5\n",
            "Epoch 61/100\n",
            "141/141 - 54s - loss: 0.2187 - val_loss: 0.5525\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.56843 to 0.55251, saving model to model.h5\n",
            "Epoch 62/100\n",
            "141/141 - 55s - loss: 0.2018 - val_loss: 0.5414\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.55251 to 0.54136, saving model to model.h5\n",
            "Epoch 63/100\n",
            "141/141 - 54s - loss: 0.1876 - val_loss: 0.5306\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.54136 to 0.53063, saving model to model.h5\n",
            "Epoch 64/100\n",
            "141/141 - 54s - loss: 0.1727 - val_loss: 0.5215\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.53063 to 0.52151, saving model to model.h5\n",
            "Epoch 65/100\n",
            "141/141 - 55s - loss: 0.1609 - val_loss: 0.5158\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.52151 to 0.51578, saving model to model.h5\n",
            "Epoch 66/100\n",
            "141/141 - 54s - loss: 0.1502 - val_loss: 0.5085\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.51578 to 0.50852, saving model to model.h5\n",
            "Epoch 67/100\n",
            "141/141 - 54s - loss: 0.1452 - val_loss: 0.5066\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.50852 to 0.50664, saving model to model.h5\n",
            "Epoch 68/100\n",
            "141/141 - 55s - loss: 0.1403 - val_loss: 0.5061\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.50664 to 0.50611, saving model to model.h5\n",
            "Epoch 69/100\n",
            "141/141 - 55s - loss: 0.1377 - val_loss: 0.5111\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.50611\n",
            "Epoch 70/100\n",
            "141/141 - 55s - loss: 0.1390 - val_loss: 0.5035\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.50611 to 0.50349, saving model to model.h5\n",
            "Epoch 71/100\n",
            "141/141 - 55s - loss: 0.1315 - val_loss: 0.4975\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.50349 to 0.49751, saving model to model.h5\n",
            "Epoch 72/100\n",
            "141/141 - 55s - loss: 0.1276 - val_loss: 0.5031\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.49751\n",
            "Epoch 73/100\n",
            "141/141 - 55s - loss: 0.1218 - val_loss: 0.4949\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.49751 to 0.49491, saving model to model.h5\n",
            "Epoch 74/100\n",
            "141/141 - 55s - loss: 0.1076 - val_loss: 0.4811\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.49491 to 0.48111, saving model to model.h5\n",
            "Epoch 75/100\n",
            "141/141 - 54s - loss: 0.0978 - val_loss: 0.4754\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.48111 to 0.47543, saving model to model.h5\n",
            "Epoch 76/100\n",
            "141/141 - 54s - loss: 0.0904 - val_loss: 0.4722\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.47543 to 0.47221, saving model to model.h5\n",
            "Epoch 77/100\n",
            "141/141 - 55s - loss: 0.0820 - val_loss: 0.4715\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.47221 to 0.47153, saving model to model.h5\n",
            "Epoch 78/100\n",
            "141/141 - 55s - loss: 0.0777 - val_loss: 0.4645\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.47153 to 0.46451, saving model to model.h5\n",
            "Epoch 79/100\n",
            "141/141 - 55s - loss: 0.0707 - val_loss: 0.4665\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.46451\n",
            "Epoch 80/100\n",
            "141/141 - 55s - loss: 0.0659 - val_loss: 0.4598\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.46451 to 0.45985, saving model to model.h5\n",
            "Epoch 81/100\n",
            "141/141 - 55s - loss: 0.0618 - val_loss: 0.4592\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.45985 to 0.45922, saving model to model.h5\n",
            "Epoch 82/100\n",
            "141/141 - 56s - loss: 0.0602 - val_loss: 0.4615\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.45922\n",
            "Epoch 83/100\n",
            "141/141 - 61s - loss: 0.0595 - val_loss: 0.4607\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.45922\n",
            "Epoch 84/100\n",
            "141/141 - 62s - loss: 0.0638 - val_loss: 0.4679\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.45922\n",
            "Epoch 85/100\n",
            "141/141 - 62s - loss: 0.0754 - val_loss: 0.4880\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.45922\n",
            "Epoch 86/100\n",
            "141/141 - 58s - loss: 0.1029 - val_loss: 0.5140\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.45922\n",
            "Epoch 87/100\n",
            "141/141 - 57s - loss: 0.1246 - val_loss: 0.5084\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.45922\n",
            "Epoch 88/100\n",
            "141/141 - 56s - loss: 0.1184 - val_loss: 0.4981\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.45922\n",
            "Epoch 89/100\n",
            "141/141 - 55s - loss: 0.0972 - val_loss: 0.4836\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.45922\n",
            "Epoch 90/100\n",
            "141/141 - 54s - loss: 0.0738 - val_loss: 0.4648\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.45922\n",
            "Epoch 91/100\n",
            "141/141 - 56s - loss: 0.0580 - val_loss: 0.4560\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.45922 to 0.45600, saving model to model.h5\n",
            "Epoch 92/100\n",
            "141/141 - 55s - loss: 0.0465 - val_loss: 0.4527\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.45600 to 0.45268, saving model to model.h5\n",
            "Epoch 93/100\n",
            "141/141 - 54s - loss: 0.0411 - val_loss: 0.4505\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.45268 to 0.45045, saving model to model.h5\n",
            "Epoch 94/100\n",
            "141/141 - 54s - loss: 0.0384 - val_loss: 0.4507\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.45045\n",
            "Epoch 95/100\n",
            "141/141 - 55s - loss: 0.0373 - val_loss: 0.4495\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.45045 to 0.44947, saving model to model.h5\n",
            "Epoch 96/100\n",
            "141/141 - 55s - loss: 0.0362 - val_loss: 0.4508\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.44947\n",
            "Epoch 97/100\n",
            "141/141 - 55s - loss: 0.0356 - val_loss: 0.4499\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.44947\n",
            "Epoch 98/100\n",
            "141/141 - 55s - loss: 0.0350 - val_loss: 0.4516\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.44947\n",
            "Epoch 99/100\n",
            "141/141 - 54s - loss: 0.0346 - val_loss: 0.4510\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.44947\n",
            "Epoch 100/100\n",
            "141/141 - 54s - loss: 0.0353 - val_loss: 0.4532\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.44947\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd65f0afcf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNmIWhu1yIb7",
        "outputId": "d900cdb0-fcd3-4b74-9064-085dad38c72a"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-arabic-both.pkl')\n",
        "train = load_clean_sentences('english-arabic-train.pkl')\n",
        "test = load_clean_sentences('english-arabic-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Arabic Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Arabic Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=300, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 3523\n",
            "English Max Length: 10\n",
            "Arabic Vocabulary Size: 10274\n",
            "Arabic Max Length: 14\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 14, 256)           2630144   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 10, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 10, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 10, 3523)          905411    \n",
            "=================================================================\n",
            "Total params: 4,586,179\n",
            "Trainable params: 4,586,179\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/300\n",
            "141/141 - 74s - loss: 4.1760 - val_loss: 3.3887\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.38865, saving model to model.h5\n",
            "Epoch 2/300\n",
            "141/141 - 66s - loss: 3.2807 - val_loss: 3.1750\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.38865 to 3.17498, saving model to model.h5\n",
            "Epoch 3/300\n",
            "141/141 - 65s - loss: 3.1461 - val_loss: 3.0842\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.17498 to 3.08418, saving model to model.h5\n",
            "Epoch 4/300\n",
            "141/141 - 66s - loss: 3.0633 - val_loss: 3.0197\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.08418 to 3.01969, saving model to model.h5\n",
            "Epoch 5/300\n",
            "141/141 - 61s - loss: 3.0107 - val_loss: 2.9755\n",
            "\n",
            "Epoch 00005: val_loss improved from 3.01969 to 2.97546, saving model to model.h5\n",
            "Epoch 6/300\n",
            "141/141 - 66s - loss: 2.9623 - val_loss: 2.9417\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.97546 to 2.94169, saving model to model.h5\n",
            "Epoch 7/300\n",
            "141/141 - 61s - loss: 2.9269 - val_loss: 2.9104\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.94169 to 2.91042, saving model to model.h5\n",
            "Epoch 8/300\n",
            "141/141 - 57s - loss: 2.8883 - val_loss: 2.8706\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.91042 to 2.87057, saving model to model.h5\n",
            "Epoch 9/300\n",
            "141/141 - 59s - loss: 2.8232 - val_loss: 2.7959\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.87057 to 2.79593, saving model to model.h5\n",
            "Epoch 10/300\n",
            "141/141 - 57s - loss: 2.7561 - val_loss: 2.7369\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.79593 to 2.73695, saving model to model.h5\n",
            "Epoch 11/300\n",
            "141/141 - 57s - loss: 2.6913 - val_loss: 2.6801\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.73695 to 2.68007, saving model to model.h5\n",
            "Epoch 12/300\n",
            "141/141 - 57s - loss: 2.6295 - val_loss: 2.6266\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.68007 to 2.62662, saving model to model.h5\n",
            "Epoch 13/300\n",
            "141/141 - 57s - loss: 2.5677 - val_loss: 2.5752\n",
            "\n",
            "Epoch 00013: val_loss improved from 2.62662 to 2.57519, saving model to model.h5\n",
            "Epoch 14/300\n",
            "141/141 - 57s - loss: 2.5029 - val_loss: 2.5026\n",
            "\n",
            "Epoch 00014: val_loss improved from 2.57519 to 2.50265, saving model to model.h5\n",
            "Epoch 15/300\n",
            "141/141 - 57s - loss: 2.4370 - val_loss: 2.4340\n",
            "\n",
            "Epoch 00015: val_loss improved from 2.50265 to 2.43401, saving model to model.h5\n",
            "Epoch 16/300\n",
            "141/141 - 57s - loss: 2.3643 - val_loss: 2.3707\n",
            "\n",
            "Epoch 00016: val_loss improved from 2.43401 to 2.37068, saving model to model.h5\n",
            "Epoch 17/300\n",
            "141/141 - 57s - loss: 2.2877 - val_loss: 2.2939\n",
            "\n",
            "Epoch 00017: val_loss improved from 2.37068 to 2.29395, saving model to model.h5\n",
            "Epoch 18/300\n",
            "141/141 - 57s - loss: 2.2065 - val_loss: 2.2178\n",
            "\n",
            "Epoch 00018: val_loss improved from 2.29395 to 2.21782, saving model to model.h5\n",
            "Epoch 19/300\n",
            "141/141 - 57s - loss: 2.1237 - val_loss: 2.1429\n",
            "\n",
            "Epoch 00019: val_loss improved from 2.21782 to 2.14286, saving model to model.h5\n",
            "Epoch 20/300\n",
            "141/141 - 57s - loss: 2.0433 - val_loss: 2.0752\n",
            "\n",
            "Epoch 00020: val_loss improved from 2.14286 to 2.07523, saving model to model.h5\n",
            "Epoch 21/300\n",
            "141/141 - 57s - loss: 1.9662 - val_loss: 2.0055\n",
            "\n",
            "Epoch 00021: val_loss improved from 2.07523 to 2.00554, saving model to model.h5\n",
            "Epoch 22/300\n",
            "141/141 - 65s - loss: 1.8870 - val_loss: 1.9292\n",
            "\n",
            "Epoch 00022: val_loss improved from 2.00554 to 1.92918, saving model to model.h5\n",
            "Epoch 23/300\n",
            "141/141 - 67s - loss: 1.8075 - val_loss: 1.8511\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.92918 to 1.85111, saving model to model.h5\n",
            "Epoch 24/300\n",
            "141/141 - 66s - loss: 1.7255 - val_loss: 1.7862\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.85111 to 1.78621, saving model to model.h5\n",
            "Epoch 25/300\n",
            "141/141 - 66s - loss: 1.6554 - val_loss: 1.7240\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.78621 to 1.72398, saving model to model.h5\n",
            "Epoch 26/300\n",
            "141/141 - 64s - loss: 1.5897 - val_loss: 1.6637\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.72398 to 1.66367, saving model to model.h5\n",
            "Epoch 27/300\n",
            "141/141 - 64s - loss: 1.5148 - val_loss: 1.5939\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.66367 to 1.59395, saving model to model.h5\n",
            "Epoch 28/300\n",
            "141/141 - 62s - loss: 1.4430 - val_loss: 1.5355\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.59395 to 1.53550, saving model to model.h5\n",
            "Epoch 29/300\n",
            "141/141 - 64s - loss: 1.3720 - val_loss: 1.4783\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.53550 to 1.47832, saving model to model.h5\n",
            "Epoch 30/300\n",
            "141/141 - 64s - loss: 1.3095 - val_loss: 1.4229\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.47832 to 1.42285, saving model to model.h5\n",
            "Epoch 31/300\n",
            "141/141 - 64s - loss: 1.2485 - val_loss: 1.3959\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.42285 to 1.39587, saving model to model.h5\n",
            "Epoch 32/300\n",
            "141/141 - 59s - loss: 1.1903 - val_loss: 1.3199\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.39587 to 1.31990, saving model to model.h5\n",
            "Epoch 33/300\n",
            "141/141 - 64s - loss: 1.1300 - val_loss: 1.2699\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.31990 to 1.26985, saving model to model.h5\n",
            "Epoch 34/300\n",
            "141/141 - 61s - loss: 1.0725 - val_loss: 1.2192\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.26985 to 1.21924, saving model to model.h5\n",
            "Epoch 35/300\n",
            "141/141 - 57s - loss: 1.0216 - val_loss: 1.1818\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.21924 to 1.18183, saving model to model.h5\n",
            "Epoch 36/300\n",
            "141/141 - 56s - loss: 0.9662 - val_loss: 1.1338\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.18183 to 1.13385, saving model to model.h5\n",
            "Epoch 37/300\n",
            "141/141 - 56s - loss: 0.9113 - val_loss: 1.0868\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.13385 to 1.08685, saving model to model.h5\n",
            "Epoch 38/300\n",
            "141/141 - 57s - loss: 0.8595 - val_loss: 1.0488\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.08685 to 1.04877, saving model to model.h5\n",
            "Epoch 39/300\n",
            "141/141 - 57s - loss: 0.8146 - val_loss: 1.0034\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.04877 to 1.00343, saving model to model.h5\n",
            "Epoch 40/300\n",
            "141/141 - 56s - loss: 0.7690 - val_loss: 0.9746\n",
            "\n",
            "Epoch 00040: val_loss improved from 1.00343 to 0.97462, saving model to model.h5\n",
            "Epoch 41/300\n",
            "141/141 - 57s - loss: 0.7308 - val_loss: 0.9412\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.97462 to 0.94122, saving model to model.h5\n",
            "Epoch 42/300\n",
            "141/141 - 57s - loss: 0.6932 - val_loss: 0.9177\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.94122 to 0.91772, saving model to model.h5\n",
            "Epoch 43/300\n",
            "141/141 - 57s - loss: 0.6544 - val_loss: 0.8794\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.91772 to 0.87937, saving model to model.h5\n",
            "Epoch 44/300\n",
            "141/141 - 57s - loss: 0.6163 - val_loss: 0.8593\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.87937 to 0.85933, saving model to model.h5\n",
            "Epoch 45/300\n",
            "141/141 - 57s - loss: 0.5795 - val_loss: 0.8230\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.85933 to 0.82295, saving model to model.h5\n",
            "Epoch 46/300\n",
            "141/141 - 57s - loss: 0.5457 - val_loss: 0.7985\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.82295 to 0.79847, saving model to model.h5\n",
            "Epoch 47/300\n",
            "141/141 - 56s - loss: 0.5163 - val_loss: 0.7864\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.79847 to 0.78637, saving model to model.h5\n",
            "Epoch 48/300\n",
            "141/141 - 57s - loss: 0.4878 - val_loss: 0.7490\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.78637 to 0.74900, saving model to model.h5\n",
            "Epoch 49/300\n",
            "141/141 - 56s - loss: 0.4589 - val_loss: 0.7290\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.74900 to 0.72903, saving model to model.h5\n",
            "Epoch 50/300\n",
            "141/141 - 57s - loss: 0.4280 - val_loss: 0.7050\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.72903 to 0.70498, saving model to model.h5\n",
            "Epoch 51/300\n",
            "141/141 - 57s - loss: 0.4002 - val_loss: 0.6856\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.70498 to 0.68562, saving model to model.h5\n",
            "Epoch 52/300\n",
            "141/141 - 56s - loss: 0.3763 - val_loss: 0.6690\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.68562 to 0.66900, saving model to model.h5\n",
            "Epoch 53/300\n",
            "141/141 - 56s - loss: 0.3545 - val_loss: 0.6579\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.66900 to 0.65785, saving model to model.h5\n",
            "Epoch 54/300\n",
            "141/141 - 58s - loss: 0.3362 - val_loss: 0.6428\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.65785 to 0.64279, saving model to model.h5\n",
            "Epoch 55/300\n",
            "141/141 - 62s - loss: 0.3197 - val_loss: 0.6229\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.64279 to 0.62291, saving model to model.h5\n",
            "Epoch 56/300\n",
            "141/141 - 57s - loss: 0.2993 - val_loss: 0.6128\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.62291 to 0.61275, saving model to model.h5\n",
            "Epoch 57/300\n",
            "141/141 - 56s - loss: 0.2828 - val_loss: 0.5953\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.61275 to 0.59528, saving model to model.h5\n",
            "Epoch 58/300\n",
            "141/141 - 57s - loss: 0.2613 - val_loss: 0.5823\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.59528 to 0.58226, saving model to model.h5\n",
            "Epoch 59/300\n",
            "141/141 - 64s - loss: 0.2454 - val_loss: 0.5703\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.58226 to 0.57033, saving model to model.h5\n",
            "Epoch 60/300\n",
            "141/141 - 56s - loss: 0.2311 - val_loss: 0.5619\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.57033 to 0.56190, saving model to model.h5\n",
            "Epoch 61/300\n",
            "141/141 - 57s - loss: 0.2174 - val_loss: 0.5539\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.56190 to 0.55389, saving model to model.h5\n",
            "Epoch 62/300\n",
            "141/141 - 57s - loss: 0.2046 - val_loss: 0.5439\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.55389 to 0.54385, saving model to model.h5\n",
            "Epoch 63/300\n",
            "141/141 - 108s - loss: 0.1928 - val_loss: 0.5385\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.54385 to 0.53853, saving model to model.h5\n",
            "Epoch 64/300\n",
            "141/141 - 106s - loss: 0.1852 - val_loss: 0.5312\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.53853 to 0.53122, saving model to model.h5\n",
            "Epoch 65/300\n",
            "141/141 - 109s - loss: 0.1729 - val_loss: 0.5216\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.53122 to 0.52162, saving model to model.h5\n",
            "Epoch 66/300\n",
            "141/141 - 101s - loss: 0.1606 - val_loss: 0.5187\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.52162 to 0.51873, saving model to model.h5\n",
            "Epoch 67/300\n",
            "141/141 - 111s - loss: 0.1511 - val_loss: 0.5086\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.51873 to 0.50863, saving model to model.h5\n",
            "Epoch 68/300\n",
            "141/141 - 100s - loss: 0.1429 - val_loss: 0.5006\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.50863 to 0.50056, saving model to model.h5\n",
            "Epoch 69/300\n",
            "141/141 - 87s - loss: 0.1353 - val_loss: 0.4984\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.50056 to 0.49842, saving model to model.h5\n",
            "Epoch 70/300\n",
            "141/141 - 80s - loss: 0.1275 - val_loss: 0.4913\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.49842 to 0.49131, saving model to model.h5\n",
            "Epoch 71/300\n",
            "141/141 - 79s - loss: 0.1182 - val_loss: 0.4874\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.49131 to 0.48743, saving model to model.h5\n",
            "Epoch 72/300\n",
            "141/141 - 76s - loss: 0.1118 - val_loss: 0.4856\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.48743 to 0.48555, saving model to model.h5\n",
            "Epoch 73/300\n",
            "141/141 - 79s - loss: 0.1103 - val_loss: 0.4911\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.48555\n",
            "Epoch 74/300\n",
            "141/141 - 78s - loss: 0.1086 - val_loss: 0.4819\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.48555 to 0.48188, saving model to model.h5\n",
            "Epoch 75/300\n",
            "141/141 - 74s - loss: 0.1021 - val_loss: 0.4817\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.48188 to 0.48168, saving model to model.h5\n",
            "Epoch 76/300\n",
            "141/141 - 76s - loss: 0.0977 - val_loss: 0.4782\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.48168 to 0.47821, saving model to model.h5\n",
            "Epoch 77/300\n",
            "141/141 - 69s - loss: 0.0928 - val_loss: 0.4777\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.47821 to 0.47773, saving model to model.h5\n",
            "Epoch 78/300\n",
            "141/141 - 79s - loss: 0.0916 - val_loss: 0.4722\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.47773 to 0.47215, saving model to model.h5\n",
            "Epoch 79/300\n",
            "141/141 - 73s - loss: 0.0853 - val_loss: 0.4682\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.47215 to 0.46820, saving model to model.h5\n",
            "Epoch 80/300\n",
            "141/141 - 57s - loss: 0.0886 - val_loss: 0.4789\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.46820\n",
            "Epoch 81/300\n",
            "141/141 - 56s - loss: 0.0880 - val_loss: 0.4763\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.46820\n",
            "Epoch 82/300\n",
            "141/141 - 56s - loss: 0.0840 - val_loss: 0.4766\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.46820\n",
            "Epoch 83/300\n",
            "141/141 - 56s - loss: 0.0805 - val_loss: 0.4713\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.46820\n",
            "Epoch 84/300\n",
            "141/141 - 56s - loss: 0.0731 - val_loss: 0.4694\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.46820\n",
            "Epoch 85/300\n",
            "141/141 - 56s - loss: 0.0676 - val_loss: 0.4621\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.46820 to 0.46210, saving model to model.h5\n",
            "Epoch 86/300\n",
            "141/141 - 57s - loss: 0.0624 - val_loss: 0.4590\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.46210 to 0.45897, saving model to model.h5\n",
            "Epoch 87/300\n",
            "141/141 - 56s - loss: 0.0581 - val_loss: 0.4561\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.45897 to 0.45607, saving model to model.h5\n",
            "Epoch 88/300\n",
            "141/141 - 56s - loss: 0.0518 - val_loss: 0.4534\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.45607 to 0.45339, saving model to model.h5\n",
            "Epoch 89/300\n",
            "141/141 - 63s - loss: 0.0468 - val_loss: 0.4500\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.45339 to 0.44997, saving model to model.h5\n",
            "Epoch 90/300\n",
            "141/141 - 63s - loss: 0.0451 - val_loss: 0.4511\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.44997\n",
            "Epoch 91/300\n",
            "141/141 - 65s - loss: 0.0430 - val_loss: 0.4502\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.44997\n",
            "Epoch 92/300\n",
            "141/141 - 63s - loss: 0.0421 - val_loss: 0.4501\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.44997\n",
            "Epoch 93/300\n",
            "141/141 - 63s - loss: 0.0415 - val_loss: 0.4509\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.44997\n",
            "Epoch 94/300\n",
            "141/141 - 63s - loss: 0.0418 - val_loss: 0.4515\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.44997\n",
            "Epoch 95/300\n",
            "141/141 - 60s - loss: 0.0426 - val_loss: 0.4565\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.44997\n",
            "Epoch 96/300\n",
            "141/141 - 63s - loss: 0.0499 - val_loss: 0.4755\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.44997\n",
            "Epoch 97/300\n",
            "141/141 - 62s - loss: 0.0914 - val_loss: 0.5338\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.44997\n",
            "Epoch 98/300\n",
            "141/141 - 62s - loss: 0.1512 - val_loss: 0.5382\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.44997\n",
            "Epoch 99/300\n",
            "141/141 - 58s - loss: 0.1374 - val_loss: 0.5002\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.44997\n",
            "Epoch 100/300\n",
            "141/141 - 62s - loss: 0.0887 - val_loss: 0.4681\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.44997\n",
            "Epoch 101/300\n",
            "141/141 - 59s - loss: 0.0544 - val_loss: 0.4548\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.44997\n",
            "Epoch 102/300\n",
            "141/141 - 62s - loss: 0.0402 - val_loss: 0.4490\n",
            "\n",
            "Epoch 00102: val_loss improved from 0.44997 to 0.44902, saving model to model.h5\n",
            "Epoch 103/300\n",
            "141/141 - 61s - loss: 0.0341 - val_loss: 0.4465\n",
            "\n",
            "Epoch 00103: val_loss improved from 0.44902 to 0.44653, saving model to model.h5\n",
            "Epoch 104/300\n",
            "141/141 - 58s - loss: 0.0320 - val_loss: 0.4471\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.44653\n",
            "Epoch 105/300\n",
            "141/141 - 63s - loss: 0.0312 - val_loss: 0.4476\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.44653\n",
            "Epoch 106/300\n",
            "141/141 - 64s - loss: 0.0304 - val_loss: 0.4465\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.44653 to 0.44652, saving model to model.h5\n",
            "Epoch 107/300\n",
            "141/141 - 61s - loss: 0.0302 - val_loss: 0.4477\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.44652\n",
            "Epoch 108/300\n",
            "141/141 - 63s - loss: 0.0296 - val_loss: 0.4488\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.44652\n",
            "Epoch 109/300\n",
            "141/141 - 63s - loss: 0.0297 - val_loss: 0.4490\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.44652\n",
            "Epoch 110/300\n",
            "141/141 - 61s - loss: 0.0294 - val_loss: 0.4491\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.44652\n",
            "Epoch 111/300\n",
            "141/141 - 57s - loss: 0.0297 - val_loss: 0.4516\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.44652\n",
            "Epoch 112/300\n",
            "141/141 - 57s - loss: 0.0290 - val_loss: 0.4518\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.44652\n",
            "Epoch 113/300\n",
            "141/141 - 57s - loss: 0.0294 - val_loss: 0.4525\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.44652\n",
            "Epoch 114/300\n",
            "141/141 - 56s - loss: 0.0291 - val_loss: 0.4534\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.44652\n",
            "Epoch 115/300\n",
            "141/141 - 56s - loss: 0.0297 - val_loss: 0.4541\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.44652\n",
            "Epoch 116/300\n",
            "141/141 - 57s - loss: 0.0313 - val_loss: 0.4599\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.44652\n",
            "Epoch 117/300\n",
            "141/141 - 57s - loss: 0.0449 - val_loss: 0.5234\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.44652\n",
            "Epoch 118/300\n",
            "141/141 - 56s - loss: 0.1050 - val_loss: 0.5386\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.44652\n",
            "Epoch 119/300\n",
            "141/141 - 56s - loss: 0.1337 - val_loss: 0.5092\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.44652\n",
            "Epoch 120/300\n",
            "141/141 - 56s - loss: 0.0851 - val_loss: 0.4726\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.44652\n",
            "Epoch 121/300\n",
            "141/141 - 60s - loss: 0.0488 - val_loss: 0.4593\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.44652\n",
            "Epoch 122/300\n",
            "141/141 - 64s - loss: 0.0359 - val_loss: 0.4546\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.44652\n",
            "Epoch 123/300\n",
            "141/141 - 62s - loss: 0.0301 - val_loss: 0.4527\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.44652\n",
            "Epoch 124/300\n",
            "141/141 - 63s - loss: 0.0276 - val_loss: 0.4523\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.44652\n",
            "Epoch 125/300\n",
            "141/141 - 62s - loss: 0.0262 - val_loss: 0.4517\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.44652\n",
            "Epoch 126/300\n",
            "141/141 - 62s - loss: 0.0255 - val_loss: 0.4525\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.44652\n",
            "Epoch 127/300\n",
            "141/141 - 61s - loss: 0.0256 - val_loss: 0.4523\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.44652\n",
            "Epoch 128/300\n",
            "141/141 - 63s - loss: 0.0254 - val_loss: 0.4527\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.44652\n",
            "Epoch 129/300\n",
            "141/141 - 58s - loss: 0.0251 - val_loss: 0.4531\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.44652\n",
            "Epoch 130/300\n",
            "141/141 - 63s - loss: 0.0250 - val_loss: 0.4534\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.44652\n",
            "Epoch 131/300\n",
            "141/141 - 65s - loss: 0.0251 - val_loss: 0.4539\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.44652\n",
            "Epoch 132/300\n",
            "141/141 - 64s - loss: 0.0249 - val_loss: 0.4561\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.44652\n",
            "Epoch 133/300\n",
            "141/141 - 66s - loss: 0.0250 - val_loss: 0.4552\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.44652\n",
            "Epoch 134/300\n",
            "141/141 - 64s - loss: 0.0256 - val_loss: 0.4548\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.44652\n",
            "Epoch 135/300\n",
            "141/141 - 63s - loss: 0.0256 - val_loss: 0.4567\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.44652\n",
            "Epoch 136/300\n",
            "141/141 - 64s - loss: 0.0264 - val_loss: 0.4598\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.44652\n",
            "Epoch 137/300\n",
            "141/141 - 61s - loss: 0.0273 - val_loss: 0.4589\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.44652\n",
            "Epoch 138/300\n",
            "141/141 - 63s - loss: 0.0286 - val_loss: 0.4665\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.44652\n",
            "Epoch 139/300\n",
            "141/141 - 60s - loss: 0.0562 - val_loss: 0.5331\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.44652\n",
            "Epoch 140/300\n",
            "141/141 - 61s - loss: 0.1158 - val_loss: 0.5408\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.44652\n",
            "Epoch 141/300\n",
            "141/141 - 60s - loss: 0.1079 - val_loss: 0.4947\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.44652\n",
            "Epoch 142/300\n",
            "141/141 - 60s - loss: 0.0604 - val_loss: 0.4697\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.44652\n",
            "Epoch 143/300\n",
            "141/141 - 57s - loss: 0.0373 - val_loss: 0.4609\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.44652\n",
            "Epoch 144/300\n",
            "141/141 - 57s - loss: 0.0287 - val_loss: 0.4584\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.44652\n",
            "Epoch 145/300\n",
            "141/141 - 57s - loss: 0.0254 - val_loss: 0.4571\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.44652\n",
            "Epoch 146/300\n",
            "141/141 - 57s - loss: 0.0241 - val_loss: 0.4569\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.44652\n",
            "Epoch 147/300\n",
            "141/141 - 57s - loss: 0.0234 - val_loss: 0.4575\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.44652\n",
            "Epoch 148/300\n",
            "141/141 - 57s - loss: 0.0232 - val_loss: 0.4574\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.44652\n",
            "Epoch 149/300\n",
            "141/141 - 57s - loss: 0.0229 - val_loss: 0.4576\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.44652\n",
            "Epoch 150/300\n",
            "141/141 - 56s - loss: 0.0229 - val_loss: 0.4578\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.44652\n",
            "Epoch 151/300\n",
            "141/141 - 57s - loss: 0.0230 - val_loss: 0.4578\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.44652\n",
            "Epoch 152/300\n",
            "141/141 - 57s - loss: 0.0228 - val_loss: 0.4609\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.44652\n",
            "Epoch 153/300\n",
            "141/141 - 56s - loss: 0.0231 - val_loss: 0.4587\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.44652\n",
            "Epoch 154/300\n",
            "141/141 - 57s - loss: 0.0233 - val_loss: 0.4592\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.44652\n",
            "Epoch 155/300\n",
            "141/141 - 57s - loss: 0.0232 - val_loss: 0.4593\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.44652\n",
            "Epoch 156/300\n",
            "141/141 - 57s - loss: 0.0235 - val_loss: 0.4616\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.44652\n",
            "Epoch 157/300\n",
            "141/141 - 57s - loss: 0.0238 - val_loss: 0.4597\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.44652\n",
            "Epoch 158/300\n",
            "141/141 - 57s - loss: 0.0237 - val_loss: 0.4611\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.44652\n",
            "Epoch 159/300\n",
            "141/141 - 57s - loss: 0.0240 - val_loss: 0.4624\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.44652\n",
            "Epoch 160/300\n",
            "141/141 - 58s - loss: 0.0239 - val_loss: 0.4637\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.44652\n",
            "Epoch 161/300\n",
            "141/141 - 57s - loss: 0.0247 - val_loss: 0.4624\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.44652\n",
            "Epoch 162/300\n",
            "141/141 - 66s - loss: 0.0266 - val_loss: 0.4660\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.44652\n",
            "Epoch 163/300\n",
            "141/141 - 72s - loss: 0.0401 - val_loss: 0.5055\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.44652\n",
            "Epoch 164/300\n",
            "141/141 - 69s - loss: 0.0960 - val_loss: 0.5516\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.44652\n",
            "Epoch 165/300\n",
            "141/141 - 68s - loss: 0.1015 - val_loss: 0.5019\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.44652\n",
            "Epoch 166/300\n",
            "141/141 - 65s - loss: 0.0610 - val_loss: 0.4801\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.44652\n",
            "Epoch 167/300\n",
            "141/141 - 68s - loss: 0.0363 - val_loss: 0.4667\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.44652\n",
            "Epoch 168/300\n",
            "141/141 - 66s - loss: 0.0264 - val_loss: 0.4627\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.44652\n",
            "Epoch 169/300\n",
            "141/141 - 66s - loss: 0.0230 - val_loss: 0.4625\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.44652\n",
            "Epoch 170/300\n",
            "141/141 - 66s - loss: 0.0225 - val_loss: 0.4612\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.44652\n",
            "Epoch 171/300\n",
            "141/141 - 63s - loss: 0.0219 - val_loss: 0.4620\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.44652\n",
            "Epoch 172/300\n",
            "141/141 - 65s - loss: 0.0217 - val_loss: 0.4611\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.44652\n",
            "Epoch 173/300\n",
            "141/141 - 65s - loss: 0.0214 - val_loss: 0.4628\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.44652\n",
            "Epoch 174/300\n",
            "141/141 - 65s - loss: 0.0217 - val_loss: 0.4622\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.44652\n",
            "Epoch 175/300\n",
            "141/141 - 67s - loss: 0.0218 - val_loss: 0.4620\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.44652\n",
            "Epoch 176/300\n",
            "141/141 - 63s - loss: 0.0218 - val_loss: 0.4624\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.44652\n",
            "Epoch 177/300\n",
            "141/141 - 64s - loss: 0.0216 - val_loss: 0.4644\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.44652\n",
            "Epoch 178/300\n",
            "141/141 - 63s - loss: 0.0216 - val_loss: 0.4637\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.44652\n",
            "Epoch 179/300\n",
            "141/141 - 64s - loss: 0.0218 - val_loss: 0.4635\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.44652\n",
            "Epoch 180/300\n",
            "141/141 - 64s - loss: 0.0219 - val_loss: 0.4644\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.44652\n",
            "Epoch 181/300\n",
            "141/141 - 57s - loss: 0.0216 - val_loss: 0.4651\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.44652\n",
            "Epoch 182/300\n",
            "141/141 - 56s - loss: 0.0219 - val_loss: 0.4649\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.44652\n",
            "Epoch 183/300\n",
            "141/141 - 57s - loss: 0.0222 - val_loss: 0.4642\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.44652\n",
            "Epoch 184/300\n",
            "141/141 - 57s - loss: 0.0227 - val_loss: 0.4670\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.44652\n",
            "Epoch 185/300\n",
            "141/141 - 57s - loss: 0.0229 - val_loss: 0.4672\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.44652\n",
            "Epoch 186/300\n",
            "141/141 - 57s - loss: 0.0233 - val_loss: 0.4667\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.44652\n",
            "Epoch 187/300\n",
            "141/141 - 57s - loss: 0.0289 - val_loss: 0.4828\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.44652\n",
            "Epoch 188/300\n",
            "141/141 - 57s - loss: 0.0546 - val_loss: 0.5385\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.44652\n",
            "Epoch 189/300\n",
            "141/141 - 57s - loss: 0.0983 - val_loss: 0.5235\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.44652\n",
            "Epoch 190/300\n",
            "141/141 - 57s - loss: 0.0793 - val_loss: 0.4904\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.44652\n",
            "Epoch 191/300\n",
            "141/141 - 58s - loss: 0.0464 - val_loss: 0.4741\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.44652\n",
            "Epoch 192/300\n",
            "141/141 - 57s - loss: 0.0305 - val_loss: 0.4691\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.44652\n",
            "Epoch 193/300\n",
            "141/141 - 64s - loss: 0.0246 - val_loss: 0.4667\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.44652\n",
            "Epoch 194/300\n",
            "141/141 - 71s - loss: 0.0220 - val_loss: 0.4660\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.44652\n",
            "Epoch 195/300\n",
            "141/141 - 72s - loss: 0.0215 - val_loss: 0.4663\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.44652\n",
            "Epoch 196/300\n",
            "141/141 - 74s - loss: 0.0212 - val_loss: 0.4658\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.44652\n",
            "Epoch 197/300\n",
            "141/141 - 69s - loss: 0.0212 - val_loss: 0.4666\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.44652\n",
            "Epoch 198/300\n",
            "141/141 - 68s - loss: 0.0208 - val_loss: 0.4673\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.44652\n",
            "Epoch 199/300\n",
            "141/141 - 66s - loss: 0.0212 - val_loss: 0.4672\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.44652\n",
            "Epoch 200/300\n",
            "141/141 - 69s - loss: 0.0210 - val_loss: 0.4673\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.44652\n",
            "Epoch 201/300\n",
            "141/141 - 71s - loss: 0.0210 - val_loss: 0.4686\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.44652\n",
            "Epoch 202/300\n",
            "141/141 - 67s - loss: 0.0208 - val_loss: 0.4684\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.44652\n",
            "Epoch 203/300\n",
            "141/141 - 64s - loss: 0.0211 - val_loss: 0.4686\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.44652\n",
            "Epoch 204/300\n",
            "141/141 - 62s - loss: 0.0208 - val_loss: 0.4680\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.44652\n",
            "Epoch 205/300\n",
            "141/141 - 69s - loss: 0.0212 - val_loss: 0.4685\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.44652\n",
            "Epoch 206/300\n",
            "141/141 - 70s - loss: 0.0213 - val_loss: 0.4701\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.44652\n",
            "Epoch 207/300\n",
            "141/141 - 67s - loss: 0.0215 - val_loss: 0.4692\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.44652\n",
            "Epoch 208/300\n",
            "141/141 - 69s - loss: 0.0215 - val_loss: 0.4702\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.44652\n",
            "Epoch 209/300\n",
            "141/141 - 65s - loss: 0.0218 - val_loss: 0.4712\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.44652\n",
            "Epoch 210/300\n",
            "141/141 - 64s - loss: 0.0242 - val_loss: 0.4733\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.44652\n",
            "Epoch 211/300\n",
            "141/141 - 65s - loss: 0.0317 - val_loss: 0.4970\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.44652\n",
            "Epoch 212/300\n",
            "141/141 - 58s - loss: 0.0573 - val_loss: 0.5150\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.44652\n",
            "Epoch 213/300\n",
            "141/141 - 58s - loss: 0.0793 - val_loss: 0.5080\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.44652\n",
            "Epoch 214/300\n",
            "141/141 - 57s - loss: 0.0560 - val_loss: 0.4857\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.44652\n",
            "Epoch 215/300\n",
            "141/141 - 58s - loss: 0.0351 - val_loss: 0.4750\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.44652\n",
            "Epoch 216/300\n",
            "141/141 - 57s - loss: 0.0258 - val_loss: 0.4737\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.44652\n",
            "Epoch 217/300\n",
            "141/141 - 57s - loss: 0.0231 - val_loss: 0.4709\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.44652\n",
            "Epoch 218/300\n",
            "141/141 - 57s - loss: 0.0214 - val_loss: 0.4698\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.44652\n",
            "Epoch 219/300\n",
            "141/141 - 57s - loss: 0.0205 - val_loss: 0.4696\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.44652\n",
            "Epoch 220/300\n",
            "141/141 - 57s - loss: 0.0203 - val_loss: 0.4702\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.44652\n",
            "Epoch 221/300\n",
            "141/141 - 59s - loss: 0.0202 - val_loss: 0.4697\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.44652\n",
            "Epoch 222/300\n",
            "141/141 - 58s - loss: 0.0200 - val_loss: 0.4707\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.44652\n",
            "Epoch 223/300\n",
            "141/141 - 59s - loss: 0.0203 - val_loss: 0.4699\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.44652\n",
            "Epoch 224/300\n",
            "141/141 - 69s - loss: 0.0200 - val_loss: 0.4704\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.44652\n",
            "Epoch 225/300\n",
            "141/141 - 73s - loss: 0.0201 - val_loss: 0.4709\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.44652\n",
            "Epoch 226/300\n",
            "141/141 - 74s - loss: 0.0201 - val_loss: 0.4713\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.44652\n",
            "Epoch 227/300\n",
            "141/141 - 68s - loss: 0.0205 - val_loss: 0.4710\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.44652\n",
            "Epoch 228/300\n",
            "141/141 - 66s - loss: 0.0202 - val_loss: 0.4734\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.44652\n",
            "Epoch 229/300\n",
            "141/141 - 65s - loss: 0.0204 - val_loss: 0.4719\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.44652\n",
            "Epoch 230/300\n",
            "141/141 - 68s - loss: 0.0205 - val_loss: 0.4723\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.44652\n",
            "Epoch 231/300\n",
            "141/141 - 66s - loss: 0.0206 - val_loss: 0.4734\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.44652\n",
            "Epoch 232/300\n",
            "141/141 - 64s - loss: 0.0208 - val_loss: 0.4723\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.44652\n",
            "Epoch 233/300\n",
            "141/141 - 66s - loss: 0.0207 - val_loss: 0.4710\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.44652\n",
            "Epoch 234/300\n",
            "141/141 - 65s - loss: 0.0210 - val_loss: 0.4737\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.44652\n",
            "Epoch 235/300\n",
            "141/141 - 68s - loss: 0.0211 - val_loss: 0.4735\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.44652\n",
            "Epoch 236/300\n",
            "141/141 - 67s - loss: 0.0210 - val_loss: 0.4741\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.44652\n",
            "Epoch 237/300\n",
            "141/141 - 65s - loss: 0.0213 - val_loss: 0.4770\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.44652\n",
            "Epoch 238/300\n",
            "141/141 - 62s - loss: 0.0253 - val_loss: 0.4935\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.44652\n",
            "Epoch 239/300\n",
            "141/141 - 65s - loss: 0.0581 - val_loss: 0.5304\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.44652\n",
            "Epoch 240/300\n",
            "141/141 - 58s - loss: 0.0910 - val_loss: 0.5169\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.44652\n",
            "Epoch 241/300\n",
            "141/141 - 67s - loss: 0.0597 - val_loss: 0.4947\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.44652\n",
            "Epoch 242/300\n",
            "141/141 - 65s - loss: 0.0364 - val_loss: 0.4810\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.44652\n",
            "Epoch 243/300\n",
            "141/141 - 57s - loss: 0.0250 - val_loss: 0.4768\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.44652\n",
            "Epoch 244/300\n",
            "141/141 - 57s - loss: 0.0216 - val_loss: 0.4772\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.44652\n",
            "Epoch 245/300\n",
            "141/141 - 57s - loss: 0.0203 - val_loss: 0.4765\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.44652\n",
            "Epoch 246/300\n",
            "141/141 - 57s - loss: 0.0198 - val_loss: 0.4779\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.44652\n",
            "Epoch 247/300\n",
            "141/141 - 57s - loss: 0.0197 - val_loss: 0.4766\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.44652\n",
            "Epoch 248/300\n",
            "141/141 - 57s - loss: 0.0196 - val_loss: 0.4762\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.44652\n",
            "Epoch 249/300\n",
            "141/141 - 57s - loss: 0.0197 - val_loss: 0.4771\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.44652\n",
            "Epoch 250/300\n",
            "141/141 - 57s - loss: 0.0198 - val_loss: 0.4768\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.44652\n",
            "Epoch 251/300\n",
            "141/141 - 57s - loss: 0.0196 - val_loss: 0.4776\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.44652\n",
            "Epoch 252/300\n",
            "141/141 - 57s - loss: 0.0196 - val_loss: 0.4780\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.44652\n",
            "Epoch 253/300\n",
            "141/141 - 57s - loss: 0.0198 - val_loss: 0.4782\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.44652\n",
            "Epoch 254/300\n",
            "141/141 - 57s - loss: 0.0196 - val_loss: 0.4779\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.44652\n",
            "Epoch 255/300\n",
            "141/141 - 62s - loss: 0.0198 - val_loss: 0.4793\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.44652\n",
            "Epoch 256/300\n",
            "141/141 - 71s - loss: 0.0197 - val_loss: 0.4783\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.44652\n",
            "Epoch 257/300\n",
            "141/141 - 77s - loss: 0.0198 - val_loss: 0.4780\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.44652\n",
            "Epoch 258/300\n",
            "141/141 - 73s - loss: 0.0198 - val_loss: 0.4782\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.44652\n",
            "Epoch 259/300\n",
            "141/141 - 73s - loss: 0.0199 - val_loss: 0.4785\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.44652\n",
            "Epoch 260/300\n",
            "141/141 - 66s - loss: 0.0201 - val_loss: 0.4786\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.44652\n",
            "Epoch 261/300\n",
            "141/141 - 73s - loss: 0.0204 - val_loss: 0.4795\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.44652\n",
            "Epoch 262/300\n",
            "141/141 - 63s - loss: 0.0207 - val_loss: 0.4788\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.44652\n",
            "Epoch 263/300\n",
            "141/141 - 67s - loss: 0.0205 - val_loss: 0.4796\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.44652\n",
            "Epoch 264/300\n",
            "141/141 - 63s - loss: 0.0208 - val_loss: 0.4800\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.44652\n",
            "Epoch 265/300\n",
            "141/141 - 66s - loss: 0.0208 - val_loss: 0.4809\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.44652\n",
            "Epoch 266/300\n",
            "141/141 - 64s - loss: 0.0226 - val_loss: 0.4827\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.44652\n",
            "Epoch 267/300\n",
            "141/141 - 69s - loss: 0.0427 - val_loss: 0.5276\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.44652\n",
            "Epoch 268/300\n",
            "141/141 - 67s - loss: 0.0844 - val_loss: 0.5189\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.44652\n",
            "Epoch 269/300\n",
            "141/141 - 66s - loss: 0.0573 - val_loss: 0.4940\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.44652\n",
            "Epoch 270/300\n",
            "141/141 - 63s - loss: 0.0334 - val_loss: 0.4836\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.44652\n",
            "Epoch 271/300\n",
            "141/141 - 67s - loss: 0.0243 - val_loss: 0.4788\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.44652\n",
            "Epoch 272/300\n",
            "141/141 - 67s - loss: 0.0207 - val_loss: 0.4788\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.44652\n",
            "Epoch 273/300\n",
            "141/141 - 60s - loss: 0.0197 - val_loss: 0.4782\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.44652\n",
            "Epoch 274/300\n",
            "141/141 - 66s - loss: 0.0195 - val_loss: 0.4786\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.44652\n",
            "Epoch 275/300\n",
            "141/141 - 58s - loss: 0.0194 - val_loss: 0.4786\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.44652\n",
            "Epoch 276/300\n",
            "141/141 - 58s - loss: 0.0192 - val_loss: 0.4786\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.44652\n",
            "Epoch 277/300\n",
            "141/141 - 58s - loss: 0.0192 - val_loss: 0.4787\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.44652\n",
            "Epoch 278/300\n",
            "141/141 - 57s - loss: 0.0192 - val_loss: 0.4790\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.44652\n",
            "Epoch 279/300\n",
            "141/141 - 57s - loss: 0.0193 - val_loss: 0.4792\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.44652\n",
            "Epoch 280/300\n",
            "141/141 - 57s - loss: 0.0192 - val_loss: 0.4784\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.44652\n",
            "Epoch 281/300\n",
            "141/141 - 57s - loss: 0.0193 - val_loss: 0.4799\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.44652\n",
            "Epoch 282/300\n",
            "141/141 - 57s - loss: 0.0194 - val_loss: 0.4798\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.44652\n",
            "Epoch 283/300\n",
            "141/141 - 57s - loss: 0.0194 - val_loss: 0.4785\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.44652\n",
            "Epoch 284/300\n",
            "141/141 - 57s - loss: 0.0194 - val_loss: 0.4793\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.44652\n",
            "Epoch 285/300\n",
            "141/141 - 57s - loss: 0.0196 - val_loss: 0.4794\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.44652\n",
            "Epoch 286/300\n",
            "141/141 - 60s - loss: 0.0195 - val_loss: 0.4795\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.44652\n",
            "Epoch 287/300\n",
            "141/141 - 65s - loss: 0.0196 - val_loss: 0.4805\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.44652\n",
            "Epoch 288/300\n",
            "141/141 - 63s - loss: 0.0199 - val_loss: 0.4810\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.44652\n",
            "Epoch 289/300\n",
            "141/141 - 65s - loss: 0.0198 - val_loss: 0.4803\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.44652\n",
            "Epoch 290/300\n",
            "141/141 - 65s - loss: 0.0201 - val_loss: 0.4810\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.44652\n",
            "Epoch 291/300\n",
            "141/141 - 65s - loss: 0.0198 - val_loss: 0.4805\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.44652\n",
            "Epoch 292/300\n",
            "141/141 - 67s - loss: 0.0198 - val_loss: 0.4800\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.44652\n",
            "Epoch 293/300\n",
            "141/141 - 66s - loss: 0.0198 - val_loss: 0.4806\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.44652\n",
            "Epoch 294/300\n",
            "141/141 - 65s - loss: 0.0201 - val_loss: 0.4820\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.44652\n",
            "Epoch 295/300\n",
            "141/141 - 66s - loss: 0.0208 - val_loss: 0.4820\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.44652\n",
            "Epoch 296/300\n",
            "141/141 - 64s - loss: 0.0219 - val_loss: 0.4864\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.44652\n",
            "Epoch 297/300\n",
            "141/141 - 66s - loss: 0.0382 - val_loss: 0.5230\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.44652\n",
            "Epoch 298/300\n",
            "141/141 - 64s - loss: 0.0869 - val_loss: 0.5326\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.44652\n",
            "Epoch 299/300\n",
            "141/141 - 64s - loss: 0.0678 - val_loss: 0.5004\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.44652\n",
            "Epoch 300/300\n",
            "141/141 - 61s - loss: 0.0360 - val_loss: 0.4832\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.44652\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f41dab90630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg85R9vNHv2a",
        "outputId": "409e1bd7-3dae-4914-e074-8956f4ef0f9d"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-arabic-both.pkl')\n",
        "train = load_clean_sentences('english-arabic-train.pkl')\n",
        "test = load_clean_sentences('english-arabic-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Arabic Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Arabic Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=200, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 3523\n",
            "English Max Length: 10\n",
            "Arabic Vocabulary Size: 10274\n",
            "Arabic Max Length: 14\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 14, 256)           2630144   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 10, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 10, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 10, 3523)          905411    \n",
            "=================================================================\n",
            "Total params: 4,586,179\n",
            "Trainable params: 4,586,179\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "141/141 - 64s - loss: 4.1861 - val_loss: 3.4032\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.40324, saving model to model.h5\n",
            "Epoch 2/200\n",
            "141/141 - 57s - loss: 3.3066 - val_loss: 3.2119\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.40324 to 3.21189, saving model to model.h5\n",
            "Epoch 3/200\n",
            "141/141 - 57s - loss: 3.1597 - val_loss: 3.0894\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.21189 to 3.08944, saving model to model.h5\n",
            "Epoch 4/200\n",
            "141/141 - 55s - loss: 3.0764 - val_loss: 3.0200\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.08944 to 3.01998, saving model to model.h5\n",
            "Epoch 5/200\n",
            "141/141 - 55s - loss: 3.0176 - val_loss: 2.9858\n",
            "\n",
            "Epoch 00005: val_loss improved from 3.01998 to 2.98580, saving model to model.h5\n",
            "Epoch 6/200\n",
            "141/141 - 56s - loss: 2.9805 - val_loss: 2.9454\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.98580 to 2.94543, saving model to model.h5\n",
            "Epoch 7/200\n",
            "141/141 - 58s - loss: 2.9360 - val_loss: 2.9154\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.94543 to 2.91543, saving model to model.h5\n",
            "Epoch 8/200\n",
            "141/141 - 55s - loss: 2.8956 - val_loss: 2.8805\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.91543 to 2.88045, saving model to model.h5\n",
            "Epoch 9/200\n",
            "141/141 - 56s - loss: 2.8623 - val_loss: 2.8489\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.88045 to 2.84892, saving model to model.h5\n",
            "Epoch 10/200\n",
            "141/141 - 55s - loss: 2.8253 - val_loss: 2.8194\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.84892 to 2.81942, saving model to model.h5\n",
            "Epoch 11/200\n",
            "141/141 - 56s - loss: 2.7706 - val_loss: 2.7506\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.81942 to 2.75057, saving model to model.h5\n",
            "Epoch 12/200\n",
            "141/141 - 58s - loss: 2.6965 - val_loss: 2.6668\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.75057 to 2.66684, saving model to model.h5\n",
            "Epoch 13/200\n",
            "141/141 - 56s - loss: 2.6164 - val_loss: 2.6012\n",
            "\n",
            "Epoch 00013: val_loss improved from 2.66684 to 2.60116, saving model to model.h5\n",
            "Epoch 14/200\n",
            "141/141 - 57s - loss: 2.5468 - val_loss: 2.5339\n",
            "\n",
            "Epoch 00014: val_loss improved from 2.60116 to 2.53392, saving model to model.h5\n",
            "Epoch 15/200\n",
            "141/141 - 57s - loss: 2.4772 - val_loss: 2.4894\n",
            "\n",
            "Epoch 00015: val_loss improved from 2.53392 to 2.48940, saving model to model.h5\n",
            "Epoch 16/200\n",
            "141/141 - 57s - loss: 2.4082 - val_loss: 2.4128\n",
            "\n",
            "Epoch 00016: val_loss improved from 2.48940 to 2.41284, saving model to model.h5\n",
            "Epoch 17/200\n",
            "141/141 - 56s - loss: 2.3413 - val_loss: 2.3532\n",
            "\n",
            "Epoch 00017: val_loss improved from 2.41284 to 2.35321, saving model to model.h5\n",
            "Epoch 18/200\n",
            "141/141 - 57s - loss: 2.2731 - val_loss: 2.2864\n",
            "\n",
            "Epoch 00018: val_loss improved from 2.35321 to 2.28639, saving model to model.h5\n",
            "Epoch 19/200\n",
            "141/141 - 56s - loss: 2.1970 - val_loss: 2.2212\n",
            "\n",
            "Epoch 00019: val_loss improved from 2.28639 to 2.22120, saving model to model.h5\n",
            "Epoch 20/200\n",
            "141/141 - 57s - loss: 2.1205 - val_loss: 2.1414\n",
            "\n",
            "Epoch 00020: val_loss improved from 2.22120 to 2.14142, saving model to model.h5\n",
            "Epoch 21/200\n",
            "141/141 - 57s - loss: 2.0460 - val_loss: 2.0745\n",
            "\n",
            "Epoch 00021: val_loss improved from 2.14142 to 2.07447, saving model to model.h5\n",
            "Epoch 22/200\n",
            "141/141 - 56s - loss: 1.9742 - val_loss: 2.0101\n",
            "\n",
            "Epoch 00022: val_loss improved from 2.07447 to 2.01013, saving model to model.h5\n",
            "Epoch 23/200\n",
            "141/141 - 58s - loss: 1.8992 - val_loss: 1.9400\n",
            "\n",
            "Epoch 00023: val_loss improved from 2.01013 to 1.93998, saving model to model.h5\n",
            "Epoch 24/200\n",
            "141/141 - 58s - loss: 1.8217 - val_loss: 1.8704\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.93998 to 1.87042, saving model to model.h5\n",
            "Epoch 25/200\n",
            "141/141 - 60s - loss: 1.7511 - val_loss: 1.8119\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.87042 to 1.81187, saving model to model.h5\n",
            "Epoch 26/200\n",
            "141/141 - 58s - loss: 1.6781 - val_loss: 1.7446\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.81187 to 1.74462, saving model to model.h5\n",
            "Epoch 27/200\n",
            "141/141 - 58s - loss: 1.6072 - val_loss: 1.6868\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.74462 to 1.68683, saving model to model.h5\n",
            "Epoch 28/200\n",
            "141/141 - 58s - loss: 1.5354 - val_loss: 1.6330\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.68683 to 1.63303, saving model to model.h5\n",
            "Epoch 29/200\n",
            "141/141 - 59s - loss: 1.4682 - val_loss: 1.5628\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.63303 to 1.56278, saving model to model.h5\n",
            "Epoch 30/200\n",
            "141/141 - 58s - loss: 1.3978 - val_loss: 1.5011\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.56278 to 1.50105, saving model to model.h5\n",
            "Epoch 31/200\n",
            "141/141 - 60s - loss: 1.3315 - val_loss: 1.4414\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.50105 to 1.44136, saving model to model.h5\n",
            "Epoch 32/200\n",
            "141/141 - 59s - loss: 1.2649 - val_loss: 1.3880\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.44136 to 1.38804, saving model to model.h5\n",
            "Epoch 33/200\n",
            "141/141 - 59s - loss: 1.2083 - val_loss: 1.3609\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.38804 to 1.36094, saving model to model.h5\n",
            "Epoch 34/200\n",
            "141/141 - 62s - loss: 1.1491 - val_loss: 1.2907\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.36094 to 1.29065, saving model to model.h5\n",
            "Epoch 35/200\n",
            "141/141 - 59s - loss: 1.0884 - val_loss: 1.2408\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.29065 to 1.24083, saving model to model.h5\n",
            "Epoch 36/200\n",
            "141/141 - 60s - loss: 1.0297 - val_loss: 1.1877\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.24083 to 1.18770, saving model to model.h5\n",
            "Epoch 37/200\n",
            "141/141 - 62s - loss: 0.9752 - val_loss: 1.1442\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.18770 to 1.14422, saving model to model.h5\n",
            "Epoch 38/200\n",
            "141/141 - 60s - loss: 0.9261 - val_loss: 1.1165\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.14422 to 1.11649, saving model to model.h5\n",
            "Epoch 39/200\n",
            "141/141 - 57s - loss: 0.8847 - val_loss: 1.0879\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.11649 to 1.08788, saving model to model.h5\n",
            "Epoch 40/200\n",
            "141/141 - 57s - loss: 0.8382 - val_loss: 1.0308\n",
            "\n",
            "Epoch 00040: val_loss improved from 1.08788 to 1.03082, saving model to model.h5\n",
            "Epoch 41/200\n",
            "141/141 - 59s - loss: 0.7844 - val_loss: 0.9899\n",
            "\n",
            "Epoch 00041: val_loss improved from 1.03082 to 0.98989, saving model to model.h5\n",
            "Epoch 42/200\n",
            "141/141 - 59s - loss: 0.7385 - val_loss: 0.9566\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.98989 to 0.95656, saving model to model.h5\n",
            "Epoch 43/200\n",
            "141/141 - 58s - loss: 0.6979 - val_loss: 0.9256\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.95656 to 0.92556, saving model to model.h5\n",
            "Epoch 44/200\n",
            "141/141 - 57s - loss: 0.6608 - val_loss: 0.8961\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.92556 to 0.89606, saving model to model.h5\n",
            "Epoch 45/200\n",
            "141/141 - 58s - loss: 0.6240 - val_loss: 0.8639\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.89606 to 0.86390, saving model to model.h5\n",
            "Epoch 46/200\n",
            "141/141 - 58s - loss: 0.5908 - val_loss: 0.8383\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.86390 to 0.83828, saving model to model.h5\n",
            "Epoch 47/200\n",
            "141/141 - 56s - loss: 0.5554 - val_loss: 0.8123\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.83828 to 0.81227, saving model to model.h5\n",
            "Epoch 48/200\n",
            "141/141 - 55s - loss: 0.5226 - val_loss: 0.7868\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.81227 to 0.78684, saving model to model.h5\n",
            "Epoch 49/200\n",
            "141/141 - 55s - loss: 0.4929 - val_loss: 0.7629\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.78684 to 0.76293, saving model to model.h5\n",
            "Epoch 50/200\n",
            "141/141 - 55s - loss: 0.4666 - val_loss: 0.7418\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.76293 to 0.74177, saving model to model.h5\n",
            "Epoch 51/200\n",
            "141/141 - 54s - loss: 0.4416 - val_loss: 0.7284\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.74177 to 0.72839, saving model to model.h5\n",
            "Epoch 52/200\n",
            "141/141 - 54s - loss: 0.4157 - val_loss: 0.7065\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.72839 to 0.70646, saving model to model.h5\n",
            "Epoch 53/200\n",
            "141/141 - 54s - loss: 0.3922 - val_loss: 0.6877\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.70646 to 0.68766, saving model to model.h5\n",
            "Epoch 54/200\n",
            "141/141 - 55s - loss: 0.3678 - val_loss: 0.6657\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.68766 to 0.66566, saving model to model.h5\n",
            "Epoch 55/200\n",
            "141/141 - 55s - loss: 0.3429 - val_loss: 0.6527\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.66566 to 0.65270, saving model to model.h5\n",
            "Epoch 56/200\n",
            "141/141 - 57s - loss: 0.3245 - val_loss: 0.6421\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.65270 to 0.64211, saving model to model.h5\n",
            "Epoch 57/200\n",
            "141/141 - 55s - loss: 0.3059 - val_loss: 0.6226\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.64211 to 0.62262, saving model to model.h5\n",
            "Epoch 58/200\n",
            "141/141 - 55s - loss: 0.2905 - val_loss: 0.6117\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.62262 to 0.61166, saving model to model.h5\n",
            "Epoch 59/200\n",
            "141/141 - 55s - loss: 0.2702 - val_loss: 0.5966\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.61166 to 0.59658, saving model to model.h5\n",
            "Epoch 60/200\n",
            "141/141 - 54s - loss: 0.2526 - val_loss: 0.5859\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.59658 to 0.58587, saving model to model.h5\n",
            "Epoch 61/200\n",
            "141/141 - 55s - loss: 0.2398 - val_loss: 0.5750\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.58587 to 0.57504, saving model to model.h5\n",
            "Epoch 62/200\n",
            "141/141 - 57s - loss: 0.2232 - val_loss: 0.5639\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.57504 to 0.56390, saving model to model.h5\n",
            "Epoch 63/200\n",
            "141/141 - 56s - loss: 0.2065 - val_loss: 0.5539\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.56390 to 0.55386, saving model to model.h5\n",
            "Epoch 64/200\n",
            "141/141 - 56s - loss: 0.1944 - val_loss: 0.5445\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.55386 to 0.54446, saving model to model.h5\n",
            "Epoch 65/200\n",
            "141/141 - 55s - loss: 0.1825 - val_loss: 0.5380\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.54446 to 0.53797, saving model to model.h5\n",
            "Epoch 66/200\n",
            "141/141 - 55s - loss: 0.1750 - val_loss: 0.5365\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.53797 to 0.53652, saving model to model.h5\n",
            "Epoch 67/200\n",
            "141/141 - 56s - loss: 0.1680 - val_loss: 0.5259\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.53652 to 0.52585, saving model to model.h5\n",
            "Epoch 68/200\n",
            "141/141 - 56s - loss: 0.1583 - val_loss: 0.5205\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.52585 to 0.52049, saving model to model.h5\n",
            "Epoch 69/200\n",
            "141/141 - 56s - loss: 0.1489 - val_loss: 0.5194\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.52049 to 0.51940, saving model to model.h5\n",
            "Epoch 70/200\n",
            "141/141 - 55s - loss: 0.1411 - val_loss: 0.5103\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.51940 to 0.51034, saving model to model.h5\n",
            "Epoch 71/200\n",
            "141/141 - 55s - loss: 0.1314 - val_loss: 0.5058\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.51034 to 0.50584, saving model to model.h5\n",
            "Epoch 72/200\n",
            "141/141 - 55s - loss: 0.1237 - val_loss: 0.5012\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.50584 to 0.50125, saving model to model.h5\n",
            "Epoch 73/200\n",
            "141/141 - 56s - loss: 0.1165 - val_loss: 0.4972\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.50125 to 0.49715, saving model to model.h5\n",
            "Epoch 74/200\n",
            "141/141 - 56s - loss: 0.1144 - val_loss: 0.4926\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.49715 to 0.49261, saving model to model.h5\n",
            "Epoch 75/200\n",
            "141/141 - 56s - loss: 0.1088 - val_loss: 0.4895\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.49261 to 0.48954, saving model to model.h5\n",
            "Epoch 76/200\n",
            "141/141 - 56s - loss: 0.0997 - val_loss: 0.4831\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.48954 to 0.48308, saving model to model.h5\n",
            "Epoch 77/200\n",
            "141/141 - 56s - loss: 0.0909 - val_loss: 0.4800\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.48308 to 0.48001, saving model to model.h5\n",
            "Epoch 78/200\n",
            "141/141 - 56s - loss: 0.0884 - val_loss: 0.4779\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.48001 to 0.47787, saving model to model.h5\n",
            "Epoch 79/200\n",
            "141/141 - 58s - loss: 0.0837 - val_loss: 0.4759\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.47787 to 0.47591, saving model to model.h5\n",
            "Epoch 80/200\n",
            "141/141 - 57s - loss: 0.0805 - val_loss: 0.4784\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.47591\n",
            "Epoch 81/200\n",
            "141/141 - 57s - loss: 0.0800 - val_loss: 0.4729\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.47591 to 0.47295, saving model to model.h5\n",
            "Epoch 82/200\n",
            "141/141 - 58s - loss: 0.0768 - val_loss: 0.4798\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.47295\n",
            "Epoch 83/200\n",
            "141/141 - 61s - loss: 0.0770 - val_loss: 0.4739\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.47295\n",
            "Epoch 84/200\n",
            "141/141 - 61s - loss: 0.0783 - val_loss: 0.4828\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.47295\n",
            "Epoch 85/200\n",
            "141/141 - 61s - loss: 0.0846 - val_loss: 0.4826\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.47295\n",
            "Epoch 86/200\n",
            "141/141 - 58s - loss: 0.0891 - val_loss: 0.4910\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.47295\n",
            "Epoch 87/200\n",
            "141/141 - 59s - loss: 0.0908 - val_loss: 0.4844\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.47295\n",
            "Epoch 88/200\n",
            "141/141 - 59s - loss: 0.0793 - val_loss: 0.4850\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.47295\n",
            "Epoch 89/200\n",
            "141/141 - 62s - loss: 0.0716 - val_loss: 0.4692\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.47295 to 0.46923, saving model to model.h5\n",
            "Epoch 90/200\n",
            "141/141 - 63s - loss: 0.0593 - val_loss: 0.4625\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.46923 to 0.46252, saving model to model.h5\n",
            "Epoch 91/200\n",
            "141/141 - 60s - loss: 0.0510 - val_loss: 0.4608\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.46252 to 0.46079, saving model to model.h5\n",
            "Epoch 92/200\n",
            "141/141 - 59s - loss: 0.0453 - val_loss: 0.4584\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.46079 to 0.45837, saving model to model.h5\n",
            "Epoch 93/200\n",
            "141/141 - 58s - loss: 0.0422 - val_loss: 0.4555\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.45837 to 0.45553, saving model to model.h5\n",
            "Epoch 94/200\n",
            "141/141 - 56s - loss: 0.0396 - val_loss: 0.4544\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.45553 to 0.45438, saving model to model.h5\n",
            "Epoch 95/200\n",
            "141/141 - 57s - loss: 0.0386 - val_loss: 0.4560\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.45438\n",
            "Epoch 96/200\n",
            "141/141 - 57s - loss: 0.0376 - val_loss: 0.4567\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.45438\n",
            "Epoch 97/200\n",
            "141/141 - 56s - loss: 0.0370 - val_loss: 0.4566\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.45438\n",
            "Epoch 98/200\n",
            "141/141 - 56s - loss: 0.0362 - val_loss: 0.4578\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.45438\n",
            "Epoch 99/200\n",
            "141/141 - 56s - loss: 0.0362 - val_loss: 0.4592\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.45438\n",
            "Epoch 100/200\n",
            "141/141 - 56s - loss: 0.0366 - val_loss: 0.4605\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.45438\n",
            "Epoch 101/200\n",
            "141/141 - 58s - loss: 0.0435 - val_loss: 0.4919\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.45438\n",
            "Epoch 102/200\n",
            "141/141 - 56s - loss: 0.1048 - val_loss: 0.5493\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.45438\n",
            "Epoch 103/200\n",
            "141/141 - 55s - loss: 0.1592 - val_loss: 0.5368\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.45438\n",
            "Epoch 104/200\n",
            "141/141 - 56s - loss: 0.1202 - val_loss: 0.4939\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.45438\n",
            "Epoch 105/200\n",
            "141/141 - 54s - loss: 0.0725 - val_loss: 0.4677\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.45438\n",
            "Epoch 106/200\n",
            "141/141 - 54s - loss: 0.0478 - val_loss: 0.4594\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.45438\n",
            "Epoch 107/200\n",
            "141/141 - 56s - loss: 0.0368 - val_loss: 0.4554\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.45438\n",
            "Epoch 108/200\n",
            "141/141 - 54s - loss: 0.0329 - val_loss: 0.4546\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.45438\n",
            "Epoch 109/200\n",
            "141/141 - 54s - loss: 0.0308 - val_loss: 0.4554\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.45438\n",
            "Epoch 110/200\n",
            "141/141 - 53s - loss: 0.0301 - val_loss: 0.4551\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.45438\n",
            "Epoch 111/200\n",
            "141/141 - 53s - loss: 0.0294 - val_loss: 0.4556\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.45438\n",
            "Epoch 112/200\n",
            "141/141 - 53s - loss: 0.0291 - val_loss: 0.4562\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.45438\n",
            "Epoch 113/200\n",
            "141/141 - 54s - loss: 0.0288 - val_loss: 0.4560\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.45438\n",
            "Epoch 114/200\n",
            "141/141 - 53s - loss: 0.0291 - val_loss: 0.4560\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.45438\n",
            "Epoch 115/200\n",
            "141/141 - 53s - loss: 0.0291 - val_loss: 0.4558\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.45438\n",
            "Epoch 116/200\n",
            "141/141 - 53s - loss: 0.0283 - val_loss: 0.4566\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.45438\n",
            "Epoch 117/200\n",
            "141/141 - 53s - loss: 0.0287 - val_loss: 0.4573\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.45438\n",
            "Epoch 118/200\n",
            "141/141 - 54s - loss: 0.0291 - val_loss: 0.4580\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.45438\n",
            "Epoch 119/200\n",
            "141/141 - 53s - loss: 0.0287 - val_loss: 0.4583\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.45438\n",
            "Epoch 120/200\n",
            "141/141 - 54s - loss: 0.0296 - val_loss: 0.4616\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.45438\n",
            "Epoch 121/200\n",
            "141/141 - 53s - loss: 0.0303 - val_loss: 0.4619\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.45438\n",
            "Epoch 122/200\n",
            "141/141 - 54s - loss: 0.0335 - val_loss: 0.4721\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.45438\n",
            "Epoch 123/200\n",
            "141/141 - 55s - loss: 0.0527 - val_loss: 0.5028\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.45438\n",
            "Epoch 124/200\n",
            "141/141 - 54s - loss: 0.1003 - val_loss: 0.5423\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.45438\n",
            "Epoch 125/200\n",
            "141/141 - 54s - loss: 0.1149 - val_loss: 0.5064\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.45438\n",
            "Epoch 126/200\n",
            "141/141 - 53s - loss: 0.0794 - val_loss: 0.4775\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.45438\n",
            "Epoch 127/200\n",
            "141/141 - 54s - loss: 0.0449 - val_loss: 0.4662\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.45438\n",
            "Epoch 128/200\n",
            "141/141 - 54s - loss: 0.0341 - val_loss: 0.4587\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.45438\n",
            "Epoch 129/200\n",
            "141/141 - 54s - loss: 0.0288 - val_loss: 0.4600\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.45438\n",
            "Epoch 130/200\n",
            "141/141 - 54s - loss: 0.0267 - val_loss: 0.4588\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.45438\n",
            "Epoch 131/200\n",
            "141/141 - 54s - loss: 0.0256 - val_loss: 0.4590\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.45438\n",
            "Epoch 132/200\n",
            "141/141 - 54s - loss: 0.0251 - val_loss: 0.4600\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.45438\n",
            "Epoch 133/200\n",
            "141/141 - 53s - loss: 0.0252 - val_loss: 0.4593\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.45438\n",
            "Epoch 134/200\n",
            "141/141 - 54s - loss: 0.0252 - val_loss: 0.4605\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.45438\n",
            "Epoch 135/200\n",
            "141/141 - 54s - loss: 0.0251 - val_loss: 0.4599\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.45438\n",
            "Epoch 136/200\n",
            "141/141 - 55s - loss: 0.0254 - val_loss: 0.4616\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.45438\n",
            "Epoch 137/200\n",
            "141/141 - 54s - loss: 0.0255 - val_loss: 0.4624\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.45438\n",
            "Epoch 138/200\n",
            "141/141 - 54s - loss: 0.0254 - val_loss: 0.4620\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.45438\n",
            "Epoch 139/200\n",
            "141/141 - 55s - loss: 0.0256 - val_loss: 0.4622\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.45438\n",
            "Epoch 140/200\n",
            "141/141 - 56s - loss: 0.0252 - val_loss: 0.4643\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.45438\n",
            "Epoch 141/200\n",
            "141/141 - 57s - loss: 0.0260 - val_loss: 0.4630\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.45438\n",
            "Epoch 142/200\n",
            "141/141 - 57s - loss: 0.0256 - val_loss: 0.4658\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.45438\n",
            "Epoch 143/200\n",
            "141/141 - 55s - loss: 0.0272 - val_loss: 0.4669\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.45438\n",
            "Epoch 144/200\n",
            "141/141 - 55s - loss: 0.0370 - val_loss: 0.4901\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.45438\n",
            "Epoch 145/200\n",
            "141/141 - 55s - loss: 0.0795 - val_loss: 0.5297\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.45438\n",
            "Epoch 146/200\n",
            "141/141 - 54s - loss: 0.1109 - val_loss: 0.5238\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.45438\n",
            "Epoch 147/200\n",
            "141/141 - 55s - loss: 0.0772 - val_loss: 0.4877\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.45438\n",
            "Epoch 148/200\n",
            "141/141 - 55s - loss: 0.0469 - val_loss: 0.4751\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.45438\n",
            "Epoch 149/200\n",
            "141/141 - 54s - loss: 0.0335 - val_loss: 0.4652\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.45438\n",
            "Epoch 150/200\n",
            "141/141 - 55s - loss: 0.0275 - val_loss: 0.4640\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.45438\n",
            "Epoch 151/200\n",
            "141/141 - 55s - loss: 0.0247 - val_loss: 0.4633\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.45438\n",
            "Epoch 152/200\n",
            "141/141 - 55s - loss: 0.0239 - val_loss: 0.4631\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.45438\n",
            "Epoch 153/200\n",
            "141/141 - 56s - loss: 0.0235 - val_loss: 0.4651\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.45438\n",
            "Epoch 154/200\n",
            "141/141 - 59s - loss: 0.0234 - val_loss: 0.4644\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.45438\n",
            "Epoch 155/200\n",
            "141/141 - 57s - loss: 0.0234 - val_loss: 0.4647\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.45438\n",
            "Epoch 156/200\n",
            "141/141 - 57s - loss: 0.0233 - val_loss: 0.4646\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.45438\n",
            "Epoch 157/200\n",
            "141/141 - 57s - loss: 0.0233 - val_loss: 0.4650\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.45438\n",
            "Epoch 158/200\n",
            "141/141 - 58s - loss: 0.0236 - val_loss: 0.4654\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.45438\n",
            "Epoch 159/200\n",
            "141/141 - 61s - loss: 0.0234 - val_loss: 0.4667\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.45438\n",
            "Epoch 160/200\n",
            "141/141 - 60s - loss: 0.0233 - val_loss: 0.4657\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.45438\n",
            "Epoch 161/200\n",
            "141/141 - 59s - loss: 0.0235 - val_loss: 0.4676\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.45438\n",
            "Epoch 162/200\n",
            "141/141 - 57s - loss: 0.0237 - val_loss: 0.4661\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.45438\n",
            "Epoch 163/200\n",
            "141/141 - 57s - loss: 0.0239 - val_loss: 0.4680\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.45438\n",
            "Epoch 164/200\n",
            "141/141 - 56s - loss: 0.0240 - val_loss: 0.4676\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.45438\n",
            "Epoch 165/200\n",
            "141/141 - 57s - loss: 0.0239 - val_loss: 0.4690\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.45438\n",
            "Epoch 166/200\n",
            "141/141 - 56s - loss: 0.0239 - val_loss: 0.4679\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.45438\n",
            "Epoch 167/200\n",
            "141/141 - 55s - loss: 0.0246 - val_loss: 0.4677\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.45438\n",
            "Epoch 168/200\n",
            "141/141 - 55s - loss: 0.0253 - val_loss: 0.4715\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.45438\n",
            "Epoch 169/200\n",
            "141/141 - 56s - loss: 0.0356 - val_loss: 0.5174\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.45438\n",
            "Epoch 170/200\n",
            "141/141 - 55s - loss: 0.0919 - val_loss: 0.5348\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.45438\n",
            "Epoch 171/200\n",
            "141/141 - 55s - loss: 0.0961 - val_loss: 0.5080\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.45438\n",
            "Epoch 172/200\n",
            "141/141 - 55s - loss: 0.0573 - val_loss: 0.4815\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.45438\n",
            "Epoch 173/200\n",
            "141/141 - 55s - loss: 0.0367 - val_loss: 0.4757\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.45438\n",
            "Epoch 174/200\n",
            "141/141 - 57s - loss: 0.0275 - val_loss: 0.4724\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.45438\n",
            "Epoch 175/200\n",
            "141/141 - 55s - loss: 0.0246 - val_loss: 0.4695\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.45438\n",
            "Epoch 176/200\n",
            "141/141 - 55s - loss: 0.0232 - val_loss: 0.4704\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.45438\n",
            "Epoch 177/200\n",
            "141/141 - 54s - loss: 0.0226 - val_loss: 0.4696\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.45438\n",
            "Epoch 178/200\n",
            "141/141 - 54s - loss: 0.0225 - val_loss: 0.4690\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.45438\n",
            "Epoch 179/200\n",
            "141/141 - 54s - loss: 0.0222 - val_loss: 0.4692\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.45438\n",
            "Epoch 180/200\n",
            "141/141 - 55s - loss: 0.0222 - val_loss: 0.4698\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.45438\n",
            "Epoch 181/200\n",
            "141/141 - 54s - loss: 0.0224 - val_loss: 0.4706\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.45438\n",
            "Epoch 182/200\n",
            "141/141 - 54s - loss: 0.0218 - val_loss: 0.4701\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.45438\n",
            "Epoch 183/200\n",
            "141/141 - 54s - loss: 0.0217 - val_loss: 0.4700\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.45438\n",
            "Epoch 184/200\n",
            "141/141 - 55s - loss: 0.0219 - val_loss: 0.4702\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.45438\n",
            "Epoch 185/200\n",
            "141/141 - 54s - loss: 0.0223 - val_loss: 0.4698\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.45438\n",
            "Epoch 186/200\n",
            "141/141 - 56s - loss: 0.0223 - val_loss: 0.4716\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.45438\n",
            "Epoch 187/200\n",
            "141/141 - 54s - loss: 0.0225 - val_loss: 0.4709\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.45438\n",
            "Epoch 188/200\n",
            "141/141 - 54s - loss: 0.0228 - val_loss: 0.4715\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.45438\n",
            "Epoch 189/200\n",
            "141/141 - 54s - loss: 0.0231 - val_loss: 0.4738\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.45438\n",
            "Epoch 190/200\n",
            "141/141 - 54s - loss: 0.0234 - val_loss: 0.4724\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.45438\n",
            "Epoch 191/200\n",
            "141/141 - 55s - loss: 0.0232 - val_loss: 0.4737\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.45438\n",
            "Epoch 192/200\n",
            "141/141 - 54s - loss: 0.0233 - val_loss: 0.4723\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.45438\n",
            "Epoch 193/200\n",
            "141/141 - 55s - loss: 0.0238 - val_loss: 0.4785\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.45438\n",
            "Epoch 194/200\n",
            "141/141 - 55s - loss: 0.0344 - val_loss: 0.5112\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.45438\n",
            "Epoch 195/200\n",
            "141/141 - 53s - loss: 0.0865 - val_loss: 0.5361\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.45438\n",
            "Epoch 196/200\n",
            "141/141 - 54s - loss: 0.0919 - val_loss: 0.5034\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.45438\n",
            "Epoch 197/200\n",
            "141/141 - 55s - loss: 0.0485 - val_loss: 0.4833\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.45438\n",
            "Epoch 198/200\n",
            "141/141 - 54s - loss: 0.0305 - val_loss: 0.4756\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.45438\n",
            "Epoch 199/200\n",
            "141/141 - 55s - loss: 0.0242 - val_loss: 0.4738\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.45438\n",
            "Epoch 200/200\n",
            "141/141 - 55s - loss: 0.0222 - val_loss: 0.4735\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.45438\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f76e03e6470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-C3Zs2ExPEe",
        "outputId": "fb222ae6-d32a-46c1-8946-e9bac3bfcdd9"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-arabic-both.pkl')\n",
        "train = load_clean_sentences('english-arabic-train.pkl')\n",
        "test = load_clean_sentences('english-arabic-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[يجب ان تنتظر], target=[You need to wait], predicted=[you need to wait]\n",
            "src=[عملت بجد الشهر الماضي], target=[I worked hard last month], predicted=[i worked hard last month]\n",
            "src=[لم أختي سيّئة إلى هذا الحد؟], target=[Why is my sister so mean], predicted=[why is my sister so mean]\n",
            "src=[التنين حيوان خرافي], target=[The dragon is an imaginary creature], predicted=[the dragon is an imaginary]\n",
            "src=[من ماذا تخاف؟], target=[What are you frightened of], predicted=[what are you frightened of]\n",
            "src=[حريّ لك الذهاب إلى الفراش], target=[Youd better go to bed], predicted=[youd better go to bed]\n",
            "src=[هل كل شيء على ما يرام؟], target=[Is everything all right], predicted=[is everything all right]\n",
            "src=[أعترف أني كنت مخطئًا], target=[I admit that I was wrong], predicted=[i admit that i was wrong]\n",
            "src=[نادراً ما تخرج أيام الأحد], target=[She rarely goes out on Sundays], predicted=[she rarely goes out on sundays]\n",
            "src=[هو ترجم الاية الى اللغة الانكليزية ], target=[He translated the verse into English], predicted=[he translated the verse into english]\n",
            "BLEU-1: 0.765468\n",
            "BLEU-2: 0.726619\n",
            "BLEU-3: 0.707334\n",
            "BLEU-4: 0.621298\n",
            "test\n",
            "src=[كتبت إلى أمي], target=[I wrote a letter to my mother], predicted=[i wrote a letter to my mother]\n",
            "src=[الباب مفتوح], target=[The door is open], predicted=[the door is open]\n",
            "src=[بدأتُ بالبكاء], target=[I began to cry], predicted=[i began to cry]\n",
            "src=[إنه جزء من النظام], target=[Its part of the system], predicted=[its part of the system]\n",
            "src=[أحضرها لي], target=[Bring it to me], predicted=[bring it to me]\n",
            "src=[أنتِ كسرتِ القواعد], target=[You broke the rules], predicted=[you broke the rules]\n",
            "src=[لا أحد يستمع إليّ], target=[Nobody listened to me], predicted=[nobody listened to me]\n",
            "src=[هذه السكين حادة جداً], target=[This knife is very sharp], predicted=[this knife is very sharp]\n",
            "src=[لا أستطيع أن أشكركِ بما فيه الكفاية], target=[I cant thank you enough], predicted=[i cant thank you enough]\n",
            "src=[لم تظن أنني أخبرتها بذلك؟], target=[Why do you think I told her about it], predicted=[why do you think i told her about it]\n",
            "BLEU-1: 0.705376\n",
            "BLEU-2: 0.662610\n",
            "BLEU-3: 0.647786\n",
            "BLEU-4: 0.562225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYc0LL-8ykHc",
        "outputId": "18bc7248-7701-46d4-d6dc-2c9e7effcb73"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-arabic-both.pkl')\n",
        "train = load_clean_sentences('english-arabic-train.pkl')\n",
        "test = load_clean_sentences('english-arabic-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[إنه جدير بأن يكون قائد فريقنا], target=[He is worthy to be captain of our team], predicted=[he is worthy to be captain of our team]\n",
            "src=[كذب علينا], target=[He lied to us], predicted=[he lied to us]\n",
            "src=[إن ذلك ليس مهماً], target=[This is not important], predicted=[this is not important]\n",
            "src=[أنا لا أحب السباحة في المياه المالحة], target=[I dont like swimming in salt water], predicted=[i dont like swimming in salt water]\n",
            "src=[لا يمكنك أن تفعل شيئين في الوقت نفسه], target=[You cant do two things at once], predicted=[you cant do two things at once]\n",
            "src=[ليس هناك أطباق نظيفة], target=[There are no clean plates], predicted=[there are no clean plates]\n",
            "src=[هل سنلحق بالقطار؟], target=[Will we be in time for the train], predicted=[will we be in time for the train]\n",
            "src=[يتحدث بسرعة], target=[He is a fast speaker], predicted=[he speaks a fast speaker]\n",
            "src=[أنا لا أحبك], target=[I dont love you], predicted=[i dont love you]\n",
            "src=[انتهت المباراة بالتعادل], target=[The match ended in a draw], predicted=[the match ended in a draw]\n",
            "BLEU-1: 0.764399\n",
            "BLEU-2: 0.724959\n",
            "BLEU-3: 0.705279\n",
            "BLEU-4: 0.618891\n",
            "test\n",
            "src=[بإمكانك وضعها في أي مكان], target=[You can put it anywhere], predicted=[you can put it anywhere]\n",
            "src=[أنا كبير بما فيه الكفاية], target=[Im old enough], predicted=[im old enough]\n",
            "src=[الرياضيات مادة صعبة بالنسبة لي], target=[Mathematics is difficult for me], predicted=[mathematics is difficult for me]\n",
            "src=[هل تمانع أن أدخن هنا؟], target=[Do you mind if I smoke here], predicted=[do you mind if i smoke here]\n",
            "src=[أتى الولد راكضاً], target=[The boy came running], predicted=[the boy came running]\n",
            "src=[هل الجو ممطر؟], target=[Is it rainy], predicted=[is it rainy]\n",
            "src=[تأخّرتُ بسبب إزدحام الطريق], target=[I was late because of the traffic], predicted=[i was late because of the traffic]\n",
            "src=[لقد نسيت شيئا، أليس كذلك؟], target=[Youre forgetting something arent you], predicted=[youre forgetting something arent you]\n",
            "src=[وصلت منذ لحظات], target=[I just arrived now], predicted=[i just arrived now]\n",
            "src=[سمعت تلك القصة من قبل], target=[I have heard the story], predicted=[i have heard the story]\n",
            "BLEU-1: 0.707203\n",
            "BLEU-2: 0.664358\n",
            "BLEU-3: 0.649248\n",
            "BLEU-4: 0.563432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnTyNFO1iIyz",
        "outputId": "81bb8726-75ff-4d6f-daea-1897975bd464"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-arabic-both.pkl')\n",
        "train = load_clean_sentences('english-arabic-train.pkl')\n",
        "test = load_clean_sentences('english-arabic-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[نريد أن نشتري منزلًا جديدًا], target=[We want to buy a new house], predicted=[we want to buy a new house]\n",
            "src=[ستبدأ المسرحية الساعة الثانية مساءً], target=[The play begins at 2 pm], predicted=[the play begins at 2 pm]\n",
            "src=[كل ابن آدم خطاّء], target=[Everyone makes mistakes], predicted=[to makes mistakes human]\n",
            "src=[لا يهتم أحد برأيك], target=[Nobody cares what you think], predicted=[nobody cares what you think]\n",
            "src=[من سرق التفاحة؟], target=[Who stole the apple], predicted=[who stole the apple]\n",
            "src=[هل بدأت تشعر بالخوف؟], target=[Are you scared already], predicted=[are you scared already]\n",
            "src=[أعترف أني مخطئ], target=[Ill admit Im wrong], predicted=[ill admit im wrong]\n",
            "src=[لم سألتني؟], target=[Why did you ask me], predicted=[why did you ask me]\n",
            "src=[هلّا نصحتني بطبيب جيّد؟], target=[Can you recommend a good doctor], predicted=[can you recommend a good doctor]\n",
            "src=[أوافقك الرأي تماما], target=[I agree with you entirely], predicted=[i agree with you entirely]\n",
            "BLEU-1: 0.765843\n",
            "BLEU-2: 0.727017\n",
            "BLEU-3: 0.707661\n",
            "BLEU-4: 0.621634\n",
            "test\n",
            "src=[إبقي نحيفة], target=[Stay thin], predicted=[stay thin]\n",
            "src=[عليك أن تدرس بجدّ], target=[You must study hard], predicted=[you must study hard]\n",
            "src=[خذ وختك], target=[Take your time], predicted=[take your time]\n",
            "src=[يجب أن أعرف لماذا تحتاج هذا], target=[I need to know why you need this], predicted=[i need to know why you need this]\n",
            "src=[تأسست مدرستنا عام ۱۹۹۰م], target=[Our school was founded in 1990], predicted=[our school was founded in 1990]\n",
            "src=[من سيعتني بالطفل ؟], target=[Who will take care of the baby], predicted=[who will take care of the baby]\n",
            "src=[يجب أن تتعلم من أخطائك], target=[You must learn from your mistakes], predicted=[you must learn from your mistakes]\n",
            "src=[أأريتها والديك؟], target=[Did you show it to your parents], predicted=[did you show it to your parents]\n",
            "src=[سأكون هناك غداً], target=[I will be there tomorrow], predicted=[i will be there tomorrow]\n",
            "src=[اختبأ توم تحت الطاولة], target=[Tom hid himself under the table], predicted=[tom hid himself the the table]\n",
            "BLEU-1: 0.708758\n",
            "BLEU-2: 0.666188\n",
            "BLEU-3: 0.650951\n",
            "BLEU-4: 0.565170\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}